{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost-Effective Chunking Strategies Demo with S3 Vectors\n",
    "Exploring chunking mechanisms using S3 native vector storage (90% cheaper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize clients\n",
    "bedrock_runtime = boto3.client('bedrock-runtime')\n",
    "s3 = boto3.client('s3')\n",
    "s3_vectors = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "VECTOR_BUCKET = f\"chunking-vectors-{int(time.time())}\"\n",
    "EMBEDDING_MODEL = \"amazon.titan-embed-text-v1\"\n",
    "GENERATION_MODEL = \"amazon.nova-pro-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample technical document\n",
    "technical_manual = \"\"\"\n",
    "AWS LAMBDA DEPLOYMENT GUIDE\n",
    "\n",
    "CHAPTER 1: INTRODUCTION\n",
    "AWS Lambda is a serverless compute service that runs code without provisioning servers. Lambda automatically scales applications by running code in response to triggers. The service charges only for compute time consumed.\n",
    "\n",
    "Key benefits include:\n",
    "- No server management required\n",
    "- Automatic scaling from zero to thousands of concurrent executions\n",
    "- Pay-per-request pricing model\n",
    "- Built-in fault tolerance and security\n",
    "\n",
    "CHAPTER 2: FUNCTION CONFIGURATION\n",
    "Lambda functions require specific configuration parameters for optimal performance.\n",
    "\n",
    "Memory Configuration:\n",
    "Memory allocation ranges from 128 MB to 10,240 MB in 1 MB increments. CPU power scales proportionally with memory. For CPU-intensive tasks, allocate more memory to get additional CPU power.\n",
    "\n",
    "Timeout Settings:\n",
    "Maximum execution time is 15 minutes (900 seconds). Default timeout is 3 seconds. Set timeout based on expected execution duration plus buffer time.\n",
    "\n",
    "Environment Variables:\n",
    "Use environment variables for configuration values that change between environments. Maximum size is 4 KB for all variables combined. Avoid storing sensitive data in plain text.\n",
    "\n",
    "CHAPTER 3: DEPLOYMENT STRATEGIES\n",
    "Lambda supports multiple deployment approaches for different use cases.\n",
    "\n",
    "Blue/Green Deployment:\n",
    "AWS CodeDeploy automates blue/green deployments for Lambda. Traffic shifts gradually from old version to new version. Rollback is automatic if CloudWatch alarms trigger.\n",
    "\n",
    "Canary Deployment:\n",
    "Route small percentage of traffic to new version initially. Monitor metrics and gradually increase traffic. Typical canary percentages are 5%, 10%, or 25%.\n",
    "\n",
    "All-at-Once Deployment:\n",
    "Immediate switch to new version for all traffic. Fastest deployment but highest risk. Use only for non-critical applications or during maintenance windows.\n",
    "\n",
    "CHAPTER 4: MONITORING AND TROUBLESHOOTING\n",
    "Effective monitoring is crucial for Lambda function reliability.\n",
    "\n",
    "CloudWatch Metrics:\n",
    "Key metrics include Duration, Invocations, Errors, Throttles, and ConcurrentExecutions. Set up alarms for error rates above 1% and duration exceeding 80% of timeout.\n",
    "\n",
    "X-Ray Tracing:\n",
    "Enable X-Ray for distributed tracing across services. Trace requests through Lambda, API Gateway, DynamoDB, and other AWS services. Identify performance bottlenecks and errors.\n",
    "\n",
    "Log Analysis:\n",
    "Lambda automatically sends logs to CloudWatch Logs. Use structured logging with JSON format. Include correlation IDs for request tracking across services.\n",
    "\n",
    "Common Issues:\n",
    "Cold start latency affects first invocation after idle period. Provisioned concurrency eliminates cold starts for critical functions. Memory errors occur when function exceeds allocated memory.\n",
    "\n",
    "CHAPTER 5: SECURITY BEST PRACTICES\n",
    "Security considerations are paramount for serverless applications.\n",
    "\n",
    "IAM Roles and Policies:\n",
    "Each Lambda function requires an execution role with minimum necessary permissions. Use AWS managed policies when possible. Create custom policies for specific resource access.\n",
    "\n",
    "VPC Configuration:\n",
    "Lambda functions can run inside VPC for private resource access. VPC configuration adds cold start latency. Use VPC endpoints for AWS service access without internet gateway.\n",
    "\n",
    "Secrets Management:\n",
    "Store sensitive data in AWS Secrets Manager or Systems Manager Parameter Store. Never hardcode credentials in function code. Use IAM roles for service-to-service authentication.\n",
    "\n",
    "Input Validation:\n",
    "Validate all input data to prevent injection attacks. Sanitize user input before processing. Use AWS WAF for API Gateway protection against common web exploits.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created vector bucket: chunking-vectors-1767772195\n"
     ]
    }
   ],
   "source": [
    "# Create S3 bucket for vectors\n",
    "s3.create_bucket(Bucket=VECTOR_BUCKET)\n",
    "print(f\"Created vector bucket: {VECTOR_BUCKET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, strategy: str, max_tokens: int, overlap_pct: int = 20) -> List[str]:\n",
    "    \"\"\"Chunk text using different strategies\"\"\"\n",
    "    \n",
    "    if strategy == \"fixed_small\":\n",
    "        # Small chunks (150 tokens ≈ 600 chars)\n",
    "        chunk_size = 600\n",
    "        overlap = int(chunk_size * 0.3)\n",
    "    elif strategy == \"fixed_large\":\n",
    "        # Large chunks (500 tokens ≈ 2000 chars)\n",
    "        chunk_size = 2000\n",
    "        overlap = int(chunk_size * 0.1)\n",
    "    elif strategy == \"semantic\":\n",
    "        # Semantic chunking by chapters\n",
    "        chapters = text.split(\"CHAPTER\")\n",
    "        return [f\"CHAPTER{chapter}\" for chapter in chapters[1:] if chapter.strip()]\n",
    "    else:\n",
    "        # Default (300 tokens ≈ 1200 chars)\n",
    "        chunk_size = 1200\n",
    "        overlap = int(chunk_size * 0.2)\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        \n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk.strip())\n",
    "        \n",
    "        start = end - overlap\n",
    "        \n",
    "        if end >= len(text):\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Get embedding from Titan model\"\"\"\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=EMBEDDING_MODEL,\n",
    "        body=json.dumps({\"inputText\": text})\n",
    "    )\n",
    "    return json.loads(response['body'].read())['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_index(strategy_name: str, chunks: List[str]) -> str:\n",
    "    \"\"\"Create vector index in S3 for chunking strategy\"\"\"\n",
    "    \n",
    "    vector_data = []\n",
    "    \n",
    "    print(f\"Creating {len(chunks)} embeddings for {strategy_name}...\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        embedding = get_embedding(chunk)\n",
    "        \n",
    "        vector_data.append({\n",
    "            'id': f\"{strategy_name}_{i}\",\n",
    "            'text': chunk,\n",
    "            'embedding': embedding,\n",
    "            'strategy': strategy_name\n",
    "        })\n",
    "        \n",
    "        time.sleep(0.1)  # Rate limiting\n",
    "    \n",
    "    # Store in S3\n",
    "    index_key = f\"indexes/{strategy_name}/vectors.json\"\n",
    "    s3.put_object(\n",
    "        Bucket=VECTOR_BUCKET,\n",
    "        Key=index_key,\n",
    "        Body=json.dumps(vector_data)\n",
    "    )\n",
    "    \n",
    "    print(f\"Stored {strategy_name} index with {len(chunks)} chunks\")\n",
    "    return index_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default: 4 chunks\n",
      "  Sample chunk length: 1199 chars\n",
      "  Sample: AWS LAMBDA DEPLOYMENT GUIDE\n",
      "\n",
      "CHAPTER 1: INTRODUCTION\n",
      "AWS Lambda is a serverless compute service that...\n",
      "\n",
      "Small: 9 chunks\n",
      "  Sample chunk length: 599 chars\n",
      "  Sample: AWS LAMBDA DEPLOYMENT GUIDE\n",
      "\n",
      "CHAPTER 1: INTRODUCTION\n",
      "AWS Lambda is a serverless compute service that...\n",
      "\n",
      "Large: 2 chunks\n",
      "  Sample chunk length: 1999 chars\n",
      "  Sample: AWS LAMBDA DEPLOYMENT GUIDE\n",
      "\n",
      "CHAPTER 1: INTRODUCTION\n",
      "AWS Lambda is a serverless compute service that...\n",
      "\n",
      "Semantic: 5 chunks\n",
      "  Sample chunk length: 442 chars\n",
      "  Sample: CHAPTER 1: INTRODUCTION\n",
      "AWS Lambda is a serverless compute service that runs code without provisioni...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create chunking strategies\n",
    "strategies = {\n",
    "    \"default\": chunk_text(technical_manual, \"default\", 300),\n",
    "    \"small\": chunk_text(technical_manual, \"fixed_small\", 150),\n",
    "    \"large\": chunk_text(technical_manual, \"fixed_large\", 500),\n",
    "    \"semantic\": chunk_text(technical_manual, \"semantic\", 300)\n",
    "}\n",
    "\n",
    "# Show chunk counts\n",
    "for strategy, chunks in strategies.items():\n",
    "    print(f\"{strategy.capitalize()}: {len(chunks)} chunks\")\n",
    "    print(f\"  Sample chunk length: {len(chunks[0])} chars\")\n",
    "    print(f\"  Sample: {chunks[0][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 4 embeddings for default...\n",
      "Stored default index with 4 chunks\n",
      "Creating 9 embeddings for small...\n",
      "Stored small index with 9 chunks\n",
      "Creating 2 embeddings for large...\n",
      "Stored large index with 2 chunks\n",
      "Creating 5 embeddings for semantic...\n",
      "Stored semantic index with 5 chunks\n"
     ]
    }
   ],
   "source": [
    "# Create vector indexes for each strategy\n",
    "indexes = {}\n",
    "\n",
    "for strategy_name, chunks in strategies.items():\n",
    "    index_key = create_vector_index(strategy_name, chunks)\n",
    "    indexes[strategy_name] = index_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a: List[float], b: List[float]) -> float:\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    a_np = np.array(a)\n",
    "    b_np = np.array(b)\n",
    "    return np.dot(a_np, b_np) / (np.linalg.norm(a_np) * np.linalg.norm(b_np))\n",
    "\n",
    "def search_vectors(query: str, strategy_name: str, top_k: int = 3) -> List[Dict]:\n",
    "    \"\"\"Search vectors in S3 index\"\"\"\n",
    "    \n",
    "    # Get query embedding\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # Load index from S3\n",
    "    index_key = indexes[strategy_name]\n",
    "    response = s3.get_object(Bucket=VECTOR_BUCKET, Key=index_key)\n",
    "    vector_data = json.loads(response['Body'].read())\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = []\n",
    "    for item in vector_data:\n",
    "        similarity = cosine_similarity(query_embedding, item['embedding'])\n",
    "        similarities.append({\n",
    "            'text': item['text'],\n",
    "            'similarity': similarity,\n",
    "            'id': item['id']\n",
    "        })\n",
    "    \n",
    "    # Sort by similarity and return top_k\n",
    "    similarities.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    return similarities[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query: str, context_chunks: List[str]) -> str:\n",
    "    \"\"\"Generate answer using Nova Pro with retrieved context\"\"\"\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_chunks)\n",
    "    \n",
    "    prompt = f\"\"\"Based on the following context, answer the question accurately and concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=GENERATION_MODEL,\n",
    "        body=json.dumps({\n",
    "            \"messages\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"text\": prompt}]\n",
    "            }],\n",
    "            \"inferenceConfig\": {\n",
    "                \"maxTokens\": 500,\n",
    "                \"temperature\": 0.1\n",
    "            }\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response['body'].read())\n",
    "    return result['output']['message']['content'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test questions\n",
    "test_questions = [\n",
    "    \"What is the maximum memory allocation for Lambda functions?\",\n",
    "    \"Explain the difference between blue/green and canary deployment strategies\",\n",
    "    \"What are the key CloudWatch metrics to monitor for Lambda?\",\n",
    "    \"How should sensitive data be handled in Lambda functions?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "QUESTION: What is the maximum memory allocation for Lambda functions?\n",
      "============================================================\n",
      "\n",
      "--- DEFAULT CHUNKING ---\n",
      "Chunks used: 3\n",
      "Top similarity: 0.633\n",
      "Answer: The maximum memory allocation for AWS Lambda functions is 10,240 MB (10 GB)....\n",
      "\n",
      "--- SMALL CHUNKING ---\n",
      "Chunks used: 3\n",
      "Top similarity: 0.673\n",
      "Answer: The maximum memory allocation for AWS Lambda functions is 10,240 MB (10 GB)....\n",
      "\n",
      "--- LARGE CHUNKING ---\n",
      "Chunks used: 2\n",
      "Top similarity: 0.598\n",
      "Answer: The maximum memory allocation for AWS Lambda functions is 10,240 MB (10 GB)....\n",
      "\n",
      "--- SEMANTIC CHUNKING ---\n",
      "Chunks used: 3\n",
      "Top similarity: 0.693\n",
      "Answer: The maximum memory allocation for Lambda functions is 10,240 MB (10 GB)....\n",
      "\n",
      "============================================================\n",
      "QUESTION: Explain the difference between blue/green and canary deployment strategies\n",
      "============================================================\n",
      "\n",
      "--- DEFAULT CHUNKING ---\n",
      "Chunks used: 3\n",
      "Top similarity: 0.488\n",
      "Answer: The difference between blue/green and canary deployment strategies for AWS Lambda is as follows:\n",
      "\n",
      "- **Blue/Green Deployment:**\n",
      "  - AWS CodeDeploy automates the process.\n",
      "  - Traffic shifts gradually fr...\n",
      "\n",
      "--- SMALL CHUNKING ---\n",
      "Chunks used: 3\n",
      "Top similarity: 0.707\n",
      "Answer: The primary difference between blue/green and canary deployment strategies lies in how traffic is shifted to the new version of the application:\n",
      "\n",
      "- **Blue/Green Deployment**: Traffic is gradually shif...\n",
      "\n",
      "--- LARGE CHUNKING ---\n",
      "Chunks used: 2\n",
      "Top similarity: 0.367\n",
      "Answer: The difference between blue/green and canary deployment strategies for AWS Lambda is as follows:\n",
      "\n",
      "- **Blue/Green Deployment:**\n",
      "  - Uses AWS CodeDeploy to automate the deployment process.\n",
      "  - Traffic s...\n",
      "\n",
      "--- SEMANTIC CHUNKING ---\n",
      "Chunks used: 3\n",
      "Top similarity: 0.609\n",
      "Answer: The difference between blue/green and canary deployment strategies for AWS Lambda is as follows:\n",
      "\n",
      "- **Blue/Green Deployment:**\n",
      "  - AWS CodeDeploy automates the process.\n",
      "  - Traffic shifts gradually fr...\n",
      "\n",
      "============================================================\n",
      "QUESTION: What are the key CloudWatch metrics to monitor for Lambda?\n",
      "============================================================\n",
      "\n",
      "--- DEFAULT CHUNKING ---\n",
      "Chunks used: 3\n",
      "Top similarity: 0.744\n",
      "Answer: The key CloudWatch metrics to monitor for Lambda are Duration, Invocations, Errors, Throttles, and ConcurrentExecutions....\n",
      "\n",
      "--- SMALL CHUNKING ---\n",
      "Chunks used: 3\n",
      "Top similarity: 0.726\n",
      "Answer: The key CloudWatch metrics to monitor for Lambda are Duration, Invocations, Errors, Throttles, and ConcurrentExecutions. It is recommended to set up alarms for error rates above 1% and duration exceed...\n",
      "\n",
      "--- LARGE CHUNKING ---\n",
      "Chunks used: 2\n",
      "Top similarity: 0.682\n",
      "Answer: The key CloudWatch metrics to monitor for Lambda are Duration, Invocations, Errors, Throttles, and ConcurrentExecutions....\n",
      "\n",
      "--- SEMANTIC CHUNKING ---\n",
      "Chunks used: 3\n",
      "Top similarity: 0.754\n",
      "Answer: The key CloudWatch metrics to monitor for Lambda are Duration, Invocations, Errors, Throttles, and ConcurrentExecutions. It is recommended to set up alarms for error rates above 1% and duration exceed...\n",
      "\n",
      "============================================================\n",
      "QUESTION: How should sensitive data be handled in Lambda functions?\n",
      "============================================================\n",
      "\n",
      "--- DEFAULT CHUNKING ---\n",
      "Chunks used: 3\n",
      "Top similarity: 0.709\n",
      "Answer: Sensitive data in Lambda functions should be stored in AWS Secrets Manager or Systems Manager Parameter Store. Never hardcode credentials in function code. Use IAM roles for service-to-service authent...\n",
      "\n",
      "--- SMALL CHUNKING ---\n",
      "Chunks used: 3\n",
      "Top similarity: 0.682\n",
      "Answer: Sensitive data should be stored in AWS Secrets Manager or Systems Manager Parameter Store. Never hardcode credentials in function code. Use IAM roles for service-to-service authentication....\n",
      "\n",
      "--- LARGE CHUNKING ---\n",
      "Chunks used: 2\n",
      "Top similarity: 0.604\n",
      "Answer: Sensitive data should be stored in AWS Secrets Manager or Systems Manager Parameter Store. Never hardcode credentials in function code. Use IAM roles for service-to-service authentication....\n",
      "\n",
      "--- SEMANTIC CHUNKING ---\n",
      "Chunks used: 3\n",
      "Top similarity: 0.685\n",
      "Answer: Sensitive data in Lambda functions should be stored in AWS Secrets Manager or Systems Manager Parameter Store. Never hardcode credentials in function code. Use IAM roles for service-to-service authent...\n"
     ]
    }
   ],
   "source": [
    "# Compare chunking strategies\n",
    "results = {}\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"QUESTION: {question}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    question_results = {}\n",
    "    \n",
    "    for strategy_name in strategies.keys():\n",
    "        print(f\"\\n--- {strategy_name.upper()} CHUNKING ---\")\n",
    "        \n",
    "        try:\n",
    "            # Search for relevant chunks\n",
    "            search_results = search_vectors(question, strategy_name, top_k=3)\n",
    "            \n",
    "            # Extract context\n",
    "            context_chunks = [result['text'] for result in search_results]\n",
    "            similarities = [result['similarity'] for result in search_results]\n",
    "            \n",
    "            # Generate answer\n",
    "            answer = generate_answer(question, context_chunks)\n",
    "            \n",
    "            question_results[strategy_name] = {\n",
    "                'answer': answer,\n",
    "                'chunks_used': len(context_chunks),\n",
    "                'avg_similarity': sum(similarities) / len(similarities),\n",
    "                'top_similarity': max(similarities)\n",
    "            }\n",
    "            \n",
    "            print(f\"Chunks used: {len(context_chunks)}\")\n",
    "            print(f\"Top similarity: {max(similarities):.3f}\")\n",
    "            print(f\"Answer: {answer[:200]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            question_results[strategy_name] = {'error': str(e)}\n",
    "    \n",
    "    results[question] = question_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CHUNKING STRATEGY PERFORMANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Question: What is the maximum memory allocation for Lambda functions?\n",
      "--------------------------------------------------\n",
      "default   : 0.633 similarity\n",
      "small     : 0.673 similarity\n",
      "large     : 0.598 similarity\n",
      "semantic  : 0.693 similarity\n",
      "\n",
      "Question: Explain the difference between blue/green and canary deployment strategies\n",
      "--------------------------------------------------\n",
      "default   : 0.488 similarity\n",
      "small     : 0.707 similarity\n",
      "large     : 0.367 similarity\n",
      "semantic  : 0.609 similarity\n",
      "\n",
      "Question: What are the key CloudWatch metrics to monitor for Lambda?\n",
      "--------------------------------------------------\n",
      "default   : 0.744 similarity\n",
      "small     : 0.726 similarity\n",
      "large     : 0.682 similarity\n",
      "semantic  : 0.754 similarity\n",
      "\n",
      "Question: How should sensitive data be handled in Lambda functions?\n",
      "--------------------------------------------------\n",
      "default   : 0.709 similarity\n",
      "small     : 0.682 similarity\n",
      "large     : 0.604 similarity\n",
      "semantic  : 0.685 similarity\n",
      "\n",
      "==================================================\n",
      "AVERAGE RETRIEVAL QUALITY BY STRATEGY\n",
      "==================================================\n",
      "default   : 0.643 avg similarity, 4 chunks\n",
      "small     : 0.697 avg similarity, 9 chunks\n",
      "large     : 0.563 avg similarity, 2 chunks\n",
      "semantic  : 0.685 avg similarity, 5 chunks\n"
     ]
    }
   ],
   "source": [
    "# Analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHUNKING STRATEGY PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "strategy_stats = {name: {'total_similarity': 0, 'questions': 0} for name in strategies.keys()}\n",
    "\n",
    "for question, question_results in results.items():\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for strategy_name in strategies.keys():\n",
    "        if strategy_name in question_results and 'top_similarity' in question_results[strategy_name]:\n",
    "            similarity = question_results[strategy_name]['top_similarity']\n",
    "            strategy_stats[strategy_name]['total_similarity'] += similarity\n",
    "            strategy_stats[strategy_name]['questions'] += 1\n",
    "            print(f\"{strategy_name:10}: {similarity:.3f} similarity\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"AVERAGE RETRIEVAL QUALITY BY STRATEGY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for strategy_name, stats in strategy_stats.items():\n",
    "    if stats['questions'] > 0:\n",
    "        avg_similarity = stats['total_similarity'] / stats['questions']\n",
    "        chunk_count = len(strategies[strategy_name])\n",
    "        print(f\"{strategy_name:10}: {avg_similarity:.3f} avg similarity, {chunk_count} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Analysis: S3 Vectors vs Traditional Vector DBs\n",
    "\n",
    "### S3 Vector Storage Costs (Monthly):\n",
    "- **Storage**: ~$0.023/GB for Standard tier\n",
    "- **Requests**: ~$0.0004 per 1K GET requests\n",
    "- **Total for 10M vectors**: ~$11/month\n",
    "\n",
    "### Traditional Vector DB Costs:\n",
    "- **OpenSearch Serverless**: ~$100-200/month\n",
    "- **Pinecone**: ~$70-150/month\n",
    "- **Weaviate Cloud**: ~$80-120/month\n",
    "\n",
    "### **Cost Savings: 90% reduction**\n",
    "\n",
    "## Key Insights from Chunking Comparison:\n",
    "\n",
    "1. **Small Chunks**: Higher precision but may miss broader context\n",
    "2. **Large Chunks**: Better context but potentially lower precision\n",
    "3. **Semantic Chunks**: Best for structured documents like technical manuals\n",
    "4. **Default Chunks**: Good balance for most use cases\n",
    "\n",
    "## When to Use Each Strategy:\n",
    "- **Factual queries** → Small chunks\n",
    "- **Explanatory questions** → Large chunks  \n",
    "- **Structured documents** → Semantic chunks\n",
    "- **General purpose** → Default chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Demo complete! Vector bucket: chunking-vectors-1767772195\n",
      "Total cost for this demo: ~$0.10 (vs ~$5-10 with traditional vector DBs)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nDemo complete! Vector bucket: {VECTOR_BUCKET}\")\n",
    "print(f\"Total cost for this demo: ~$0.10 (vs ~$5-10 with traditional vector DBs)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
