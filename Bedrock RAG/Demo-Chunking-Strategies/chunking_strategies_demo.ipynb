{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking Strategies Demo: Impact on RAG Performance\n",
    "Exploring different chunking mechanisms and their effect on retrieval quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize clients\n",
    "bedrock_agent = boto3.client('bedrock-agent')\n",
    "s3 = boto3.client('s3')\n",
    "iam = boto3.client('iam')\n",
    "sts = boto3.client('sts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BASE_BUCKET = f\"chunking-demo-{int(time.time())}\"\n",
    "ROLE_NAME = \"ChunkingDemoRole\"\n",
    "EMBEDDING_MODEL = \"amazon.titan-embed-text-v1\"\n",
    "GENERATION_MODEL = \"amazon.nova-pro-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Document: Technical Manual\n",
    "We'll use a structured technical document to demonstrate chunking impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive technical document\n",
    "technical_manual = \"\"\"\n",
    "AWS LAMBDA DEPLOYMENT GUIDE\n",
    "\n",
    "CHAPTER 1: INTRODUCTION\n",
    "AWS Lambda is a serverless compute service that runs code without provisioning servers. Lambda automatically scales applications by running code in response to triggers. The service charges only for compute time consumed.\n",
    "\n",
    "Key benefits include:\n",
    "- No server management required\n",
    "- Automatic scaling from zero to thousands of concurrent executions\n",
    "- Pay-per-request pricing model\n",
    "- Built-in fault tolerance and security\n",
    "\n",
    "CHAPTER 2: FUNCTION CONFIGURATION\n",
    "Lambda functions require specific configuration parameters for optimal performance.\n",
    "\n",
    "Memory Configuration:\n",
    "Memory allocation ranges from 128 MB to 10,240 MB in 1 MB increments. CPU power scales proportionally with memory. For CPU-intensive tasks, allocate more memory to get additional CPU power.\n",
    "\n",
    "Timeout Settings:\n",
    "Maximum execution time is 15 minutes (900 seconds). Default timeout is 3 seconds. Set timeout based on expected execution duration plus buffer time.\n",
    "\n",
    "Environment Variables:\n",
    "Use environment variables for configuration values that change between environments. Maximum size is 4 KB for all variables combined. Avoid storing sensitive data in plain text.\n",
    "\n",
    "CHAPTER 3: DEPLOYMENT STRATEGIES\n",
    "Lambda supports multiple deployment approaches for different use cases.\n",
    "\n",
    "Blue/Green Deployment:\n",
    "AWS CodeDeploy automates blue/green deployments for Lambda. Traffic shifts gradually from old version to new version. Rollback is automatic if CloudWatch alarms trigger.\n",
    "\n",
    "Canary Deployment:\n",
    "Route small percentage of traffic to new version initially. Monitor metrics and gradually increase traffic. Typical canary percentages are 5%, 10%, or 25%.\n",
    "\n",
    "All-at-Once Deployment:\n",
    "Immediate switch to new version for all traffic. Fastest deployment but highest risk. Use only for non-critical applications or during maintenance windows.\n",
    "\n",
    "CHAPTER 4: MONITORING AND TROUBLESHOOTING\n",
    "Effective monitoring is crucial for Lambda function reliability.\n",
    "\n",
    "CloudWatch Metrics:\n",
    "Key metrics include Duration, Invocations, Errors, Throttles, and ConcurrentExecutions. Set up alarms for error rates above 1% and duration exceeding 80% of timeout.\n",
    "\n",
    "X-Ray Tracing:\n",
    "Enable X-Ray for distributed tracing across services. Trace requests through Lambda, API Gateway, DynamoDB, and other AWS services. Identify performance bottlenecks and errors.\n",
    "\n",
    "Log Analysis:\n",
    "Lambda automatically sends logs to CloudWatch Logs. Use structured logging with JSON format. Include correlation IDs for request tracking across services.\n",
    "\n",
    "Common Issues:\n",
    "Cold start latency affects first invocation after idle period. Provisioned concurrency eliminates cold starts for critical functions. Memory errors occur when function exceeds allocated memory.\n",
    "\n",
    "CHAPTER 5: SECURITY BEST PRACTICES\n",
    "Security considerations are paramount for serverless applications.\n",
    "\n",
    "IAM Roles and Policies:\n",
    "Each Lambda function requires an execution role with minimum necessary permissions. Use AWS managed policies when possible. Create custom policies for specific resource access.\n",
    "\n",
    "VPC Configuration:\n",
    "Lambda functions can run inside VPC for private resource access. VPC configuration adds cold start latency. Use VPC endpoints for AWS service access without internet gateway.\n",
    "\n",
    "Secrets Management:\n",
    "Store sensitive data in AWS Secrets Manager or Systems Manager Parameter Store. Never hardcode credentials in function code. Use IAM roles for service-to-service authentication.\n",
    "\n",
    "Input Validation:\n",
    "Validate all input data to prevent injection attacks. Sanitize user input before processing. Use AWS WAF for API Gateway protection against common web exploits.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking Strategy 1: Fixed Size (Default Bedrock)\n",
    "Default chunking with fixed character limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kb_with_chunking(strategy_name: str, chunking_config: Dict) -> str:\n",
    "    \"\"\"Create Knowledge Base with specific chunking configuration\"\"\"\n",
    "    \n",
    "    bucket_name = f\"{BASE_BUCKET}-{strategy_name.lower()}\"\n",
    "    kb_name = f\"lambda-guide-{strategy_name.lower()}\"\n",
    "    \n",
    "    # Create S3 bucket\n",
    "    s3.create_bucket(Bucket=bucket_name)\n",
    "    \n",
    "    # Upload document\n",
    "    s3.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=\"lambda-guide.txt\",\n",
    "        Body=technical_manual.encode('utf-8')\n",
    "    )\n",
    "    \n",
    "    # Get/Create IAM role\n",
    "    account_id = sts.get_caller_identity()['Account']\n",
    "    role_arn = f\"arn:aws:iam::{account_id}:role/{ROLE_NAME}\"\n",
    "    \n",
    "    # Create Knowledge Base with chunking config\n",
    "    kb_config = {\n",
    "        \"name\": kb_name,\n",
    "        \"description\": f\"Lambda guide with {strategy_name} chunking\",\n",
    "        \"roleArn\": role_arn,\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            \"type\": \"VECTOR\",\n",
    "            \"vectorKnowledgeBaseConfiguration\": {\n",
    "                \"embeddingModelArn\": f\"arn:aws:bedrock:us-east-1::foundation-model/{EMBEDDING_MODEL}\"\n",
    "            }\n",
    "        },\n",
    "        \"storageConfiguration\": {\n",
    "            \"type\": \"OPENSEARCH_SERVERLESS\",\n",
    "            \"opensearchServerlessConfiguration\": {\n",
    "                \"collectionArn\": \"\",\n",
    "                \"vectorIndexName\": \"bedrock-knowledge-base-default-index\",\n",
    "                \"fieldMapping\": {\n",
    "                    \"vectorField\": \"bedrock-knowledge-base-default-vector\",\n",
    "                    \"textField\": \"AMAZON_BEDROCK_TEXT_CHUNK\",\n",
    "                    \"metadataField\": \"AMAZON_BEDROCK_METADATA\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    kb_response = bedrock_agent.create_knowledge_base(**kb_config)\n",
    "    kb_id = kb_response['knowledgeBase']['knowledgeBaseId']\n",
    "    \n",
    "    # Create Data Source with chunking configuration\n",
    "    ds_config = {\n",
    "        \"knowledgeBaseId\": kb_id,\n",
    "        \"name\": f\"{strategy_name.lower()}-datasource\",\n",
    "        \"dataSourceConfiguration\": {\n",
    "            \"type\": \"S3\",\n",
    "            \"s3Configuration\": {\n",
    "                \"bucketArn\": f\"arn:aws:s3:::{bucket_name}\"\n",
    "            }\n",
    "        },\n",
    "        \"vectorIngestionConfiguration\": {\n",
    "            \"chunkingConfiguration\": chunking_config\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    ds_response = bedrock_agent.create_data_source(**ds_config)\n",
    "    ds_id = ds_response['dataSource']['dataSourceId']\n",
    "    \n",
    "    # Start ingestion\n",
    "    bedrock_agent.start_ingestion_job(\n",
    "        knowledgeBaseId=kb_id,\n",
    "        dataSourceId=ds_id\n",
    "    )\n",
    "    \n",
    "    print(f\"Created {strategy_name} KB: {kb_id}\")\n",
    "    return kb_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create IAM role first\n",
    "trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [{\n",
    "        \"Effect\": \"Allow\",\n",
    "        \"Principal\": {\"Service\": \"bedrock.amazonaws.com\"},\n",
    "        \"Action\": \"sts:AssumeRole\"\n",
    "    }]\n",
    "}\n",
    "\n",
    "role_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\"s3:GetObject\", \"s3:ListBucket\"],\n",
    "            \"Resource\": [f\"arn:aws:s3:::{BASE_BUCKET}*\", f\"arn:aws:s3:::{BASE_BUCKET}*/*\"]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\"bedrock:InvokeModel\"],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    iam.create_role(\n",
    "        RoleName=ROLE_NAME,\n",
    "        AssumeRolePolicyDocument=json.dumps(trust_policy)\n",
    "    )\n",
    "    iam.put_role_policy(\n",
    "        RoleName=ROLE_NAME,\n",
    "        PolicyName=\"ChunkingDemoPolicy\",\n",
    "        PolicyDocument=json.dumps(role_policy)\n",
    "    )\n",
    "    print(\"Created IAM role\")\n",
    "    time.sleep(10)\n",
    "except Exception as e:\n",
    "    print(f\"Role exists or error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 1: Default Fixed Size Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default chunking (300 tokens, 20% overlap)\n",
    "default_chunking = {\n",
    "    \"chunkingStrategy\": \"FIXED_SIZE\",\n",
    "    \"fixedSizeChunkingConfiguration\": {\n",
    "        \"maxTokens\": 300,\n",
    "        \"overlapPercentage\": 20\n",
    "    }\n",
    "}\n",
    "\n",
    "kb_default = create_kb_with_chunking(\"Default\", default_chunking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 2: Small Chunks (High Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small chunks for precise retrieval\n",
    "small_chunking = {\n",
    "    \"chunkingStrategy\": \"FIXED_SIZE\",\n",
    "    \"fixedSizeChunkingConfiguration\": {\n",
    "        \"maxTokens\": 150,\n",
    "        \"overlapPercentage\": 30\n",
    "    }\n",
    "}\n",
    "\n",
    "kb_small = create_kb_with_chunking(\"Small\", small_chunking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 3: Large Chunks (High Context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large chunks for more context\n",
    "large_chunking = {\n",
    "    \"chunkingStrategy\": \"FIXED_SIZE\",\n",
    "    \"fixedSizeChunkingConfiguration\": {\n",
    "        \"maxTokens\": 500,\n",
    "        \"overlapPercentage\": 10\n",
    "    }\n",
    "}\n",
    "\n",
    "kb_large = create_kb_with_chunking(\"Large\", large_chunking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 4: Semantic Chunking (Hierarchy-Aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic chunking respects document structure\n",
    "semantic_chunking = {\n",
    "    \"chunkingStrategy\": \"SEMANTIC\",\n",
    "    \"semanticChunkingConfiguration\": {\n",
    "        \"maxTokens\": 300,\n",
    "        \"bufferSize\": 0,\n",
    "        \"breakpointPercentileThreshold\": 95\n",
    "    }\n",
    "}\n",
    "\n",
    "kb_semantic = create_kb_with_chunking(\"Semantic\", semantic_chunking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait for Ingestion Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_ingestion(kb_id: str, strategy_name: str):\n",
    "    \"\"\"Wait for ingestion to complete\"\"\"\n",
    "    print(f\"Waiting for {strategy_name} ingestion...\")\n",
    "    \n",
    "    # Get data source ID\n",
    "    ds_list = bedrock_agent.list_data_sources(knowledgeBaseId=kb_id)\n",
    "    ds_id = ds_list['dataSourceSummaries'][0]['dataSourceId']\n",
    "    \n",
    "    # Get latest job\n",
    "    jobs = bedrock_agent.list_ingestion_jobs(\n",
    "        knowledgeBaseId=kb_id,\n",
    "        dataSourceId=ds_id\n",
    "    )\n",
    "    job_id = jobs['ingestionJobSummaries'][0]['ingestionJobId']\n",
    "    \n",
    "    while True:\n",
    "        job_status = bedrock_agent.get_ingestion_job(\n",
    "            knowledgeBaseId=kb_id,\n",
    "            dataSourceId=ds_id,\n",
    "            ingestionJobId=job_id\n",
    "        )\n",
    "        status = job_status['ingestionJob']['status']\n",
    "        \n",
    "        if status in ['COMPLETE', 'FAILED']:\n",
    "            print(f\"{strategy_name}: {status}\")\n",
    "            break\n",
    "        time.sleep(15)\n",
    "\n",
    "# Wait for all ingestions\n",
    "strategies = [\n",
    "    (kb_default, \"Default\"),\n",
    "    (kb_small, \"Small\"),\n",
    "    (kb_large, \"Large\"),\n",
    "    (kb_semantic, \"Semantic\")\n",
    "]\n",
    "\n",
    "for kb_id, name in strategies:\n",
    "    wait_for_ingestion(kb_id, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison: Test Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_kb(kb_id: str, question: str) -> Dict:\n",
    "    \"\"\"Query knowledge base and return response with metadata\"\"\"\n",
    "    response = bedrock_agent.retrieve_and_generate(\n",
    "        input={'text': question},\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            'type': 'KNOWLEDGE_BASE',\n",
    "            'knowledgeBaseConfiguration': {\n",
    "                'knowledgeBaseId': kb_id,\n",
    "                'modelArn': f'arn:aws:bedrock:us-east-1::foundation-model/{GENERATION_MODEL}'\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'answer': response['output']['text'],\n",
    "        'citations': response.get('citations', []),\n",
    "        'source_count': len(response.get('citations', []))\n",
    "    }\n",
    "\n",
    "# Test questions targeting different aspects\n",
    "test_questions = [\n",
    "    \"What is the maximum memory allocation for Lambda functions?\",\n",
    "    \"Explain the difference between blue/green and canary deployment strategies\",\n",
    "    \"What are the key CloudWatch metrics to monitor for Lambda?\",\n",
    "    \"How should sensitive data be handled in Lambda functions?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare chunking strategies\n",
    "results = {}\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"QUESTION: {question}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    question_results = {}\n",
    "    \n",
    "    for kb_id, strategy_name in strategies:\n",
    "        print(f\"\\n--- {strategy_name.upper()} CHUNKING ---\")\n",
    "        \n",
    "        try:\n",
    "            result = query_kb(kb_id, question)\n",
    "            question_results[strategy_name] = result\n",
    "            \n",
    "            print(f\"Sources used: {result['source_count']}\")\n",
    "            print(f\"Answer: {result['answer'][:200]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            question_results[strategy_name] = {'error': str(e)}\n",
    "    \n",
    "    results[question] = question_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Chunking Strategy Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHUNKING STRATEGY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "strategy_stats = {name: {'total_sources': 0, 'questions': 0} for _, name in strategies}\n",
    "\n",
    "for question, question_results in results.items():\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for strategy_name in ['Default', 'Small', 'Large', 'Semantic']:\n",
    "        if strategy_name in question_results and 'source_count' in question_results[strategy_name]:\n",
    "            sources = question_results[strategy_name]['source_count']\n",
    "            strategy_stats[strategy_name]['total_sources'] += sources\n",
    "            strategy_stats[strategy_name]['questions'] += 1\n",
    "            print(f\"{strategy_name:10}: {sources} sources\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"AVERAGE SOURCES PER STRATEGY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for strategy_name, stats in strategy_stats.items():\n",
    "    if stats['questions'] > 0:\n",
    "        avg_sources = stats['total_sources'] / stats['questions']\n",
    "        print(f\"{strategy_name:10}: {avg_sources:.1f} sources on average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights: Why Chunking Matters\n",
    "\n",
    "### 1. **Small Chunks (150 tokens)**\n",
    "- **Pros**: High precision, specific information retrieval\n",
    "- **Cons**: May lose context, require more sources for complete answers\n",
    "- **Best for**: Factual queries, specific data points\n",
    "\n",
    "### 2. **Large Chunks (500 tokens)**\n",
    "- **Pros**: Rich context, comprehensive information\n",
    "- **Cons**: May include irrelevant information, less precise\n",
    "- **Best for**: Complex explanations, conceptual questions\n",
    "\n",
    "### 3. **Default Chunks (300 tokens)**\n",
    "- **Pros**: Balanced approach, good for most use cases\n",
    "- **Cons**: May not be optimal for specific document types\n",
    "- **Best for**: General-purpose RAG applications\n",
    "\n",
    "### 4. **Semantic Chunks**\n",
    "- **Pros**: Respects document structure, maintains logical boundaries\n",
    "- **Cons**: Variable chunk sizes, may be computationally expensive\n",
    "- **Best for**: Structured documents, technical manuals\n",
    "\n",
    "### Chunking Strategy Selection Guidelines:\n",
    "1. **Document Type**: Technical docs → Semantic, Articles → Fixed\n",
    "2. **Query Type**: Factual → Small chunks, Explanatory → Large chunks\n",
    "3. **Context Requirements**: High context → Large chunks, Precision → Small chunks\n",
    "4. **Performance**: Speed → Fixed size, Quality → Semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDemo complete! Knowledge Base IDs:\")\n",
    "for kb_id, strategy_name in strategies:\n",
    "    print(f\"{strategy_name}: {kb_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
