{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 4: RAG with Re-ranking Pipeline\n",
    "Pattern: Advanced Modular RAG with Quality Improvement\n",
    "\n",
    "**Pipeline:**\n",
    "1. Initial retrieval (top 20 candidates)\n",
    "2. Re-ranker model (refine to top 5)\n",
    "3. Generation with high-quality context\n",
    "4. Relevance scoring throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import List, Dict, Tuple\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize clients\n",
    "bedrock_runtime = boto3.client('bedrock-runtime')\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Configuration\n",
    "RERANK_BUCKET = f\"reranking-demo-{int(time.time())}\"\n",
    "EMBEDDING_MODEL = \"amazon.titan-embed-text-v1\"\n",
    "RERANK_MODEL = \"amazon.rerank-v1:0\"  # Simulated - using Nova for re-ranking\n",
    "GENERATION_MODEL = \"amazon.nova-pro-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create S3 bucket\n",
    "s3.create_bucket(Bucket=RERANK_BUCKET)\n",
    "print(f\"Created re-ranking demo bucket: {RERANK_BUCKET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended document collection for re-ranking demo\n",
    "documents = [\n",
    "    {\"id\": \"lambda_pricing\", \"title\": \"AWS Lambda Pricing\", \"content\": \"AWS Lambda pricing is based on requests and compute time. You pay $0.20 per 1M requests and $0.0000166667 per GB-second. Free tier includes 1M requests monthly.\"},\n",
    "    {\"id\": \"lambda_memory\", \"title\": \"Lambda Memory Configuration\", \"content\": \"Configure Lambda memory from 128 MB to 10,240 MB. CPU power scales with memory allocation. Higher memory improves performance but increases cost.\"},\n",
    "    {\"id\": \"lambda_timeout\", \"title\": \"Lambda Timeout Settings\", \"content\": \"Lambda maximum execution time is 15 minutes (900 seconds). Default timeout is 3 seconds. Configure based on function requirements.\"},\n",
    "    {\"id\": \"lambda_coldstart\", \"title\": \"Lambda Cold Start Optimization\", \"content\": \"Cold starts add latency when Lambda initializes execution environments. Use provisioned concurrency and optimize package size to reduce cold starts.\"},\n",
    "    {\"id\": \"lambda_vpc\", \"title\": \"Lambda VPC Configuration\", \"content\": \"Lambda functions can access VPC resources like RDS databases. VPC configuration adds cold start latency. Use VPC endpoints for AWS services.\"},\n",
    "    {\"id\": \"lambda_monitoring\", \"title\": \"Lambda Monitoring\", \"content\": \"Monitor Lambda with CloudWatch metrics: Duration, Invocations, Errors, Throttles. Enable X-Ray tracing for distributed systems.\"},\n",
    "    {\"id\": \"lambda_security\", \"title\": \"Lambda Security\", \"content\": \"Use IAM roles with least privilege. Store secrets in AWS Secrets Manager. Enable encryption and validate input data.\"},\n",
    "    {\"id\": \"lambda_deployment\", \"title\": \"Lambda Deployment\", \"content\": \"Deploy Lambda using blue/green, canary, or all-at-once strategies. Use AWS CodeDeploy for automated deployments.\"},\n",
    "    {\"id\": \"lambda_layers\", \"title\": \"Lambda Layers\", \"content\": \"Lambda layers allow sharing code and dependencies across functions. Layers reduce deployment package size and enable code reuse.\"},\n",
    "    {\"id\": \"lambda_triggers\", \"title\": \"Lambda Triggers\", \"content\": \"Lambda functions can be triggered by S3 events, API Gateway, DynamoDB streams, SQS queues, and many other AWS services.\"},\n",
    "    {\"id\": \"lambda_env_vars\", \"title\": \"Lambda Environment Variables\", \"content\": \"Environment variables store configuration data. Maximum size is 4 KB. Use Systems Manager Parameter Store for larger configurations.\"},\n",
    "    {\"id\": \"lambda_concurrency\", \"title\": \"Lambda Concurrency\", \"content\": \"Lambda automatically scales to handle concurrent executions. Set reserved concurrency to limit scaling. Use provisioned concurrency for consistent performance.\"},\n",
    "    {\"id\": \"lambda_errors\", \"title\": \"Lambda Error Handling\", \"content\": \"Handle errors with try-catch blocks, dead letter queues, and retry policies. Monitor error rates and set up CloudWatch alarms.\"},\n",
    "    {\"id\": \"lambda_performance\", \"title\": \"Lambda Performance Optimization\", \"content\": \"Optimize Lambda performance by right-sizing memory, minimizing cold starts, using connection pooling, and efficient code practices.\"},\n",
    "    {\"id\": \"lambda_costs\", \"title\": \"Lambda Cost Optimization\", \"content\": \"Optimize Lambda costs by right-sizing memory allocation, reducing execution time, using ARM processors, and monitoring usage patterns.\"},\n",
    "    {\"id\": \"lambda_best_practices\", \"title\": \"Lambda Best Practices\", \"content\": \"Follow Lambda best practices: separate business logic from handler, use environment variables, implement proper logging, and design for idempotency.\"},\n",
    "    {\"id\": \"lambda_testing\", \"title\": \"Lambda Testing\", \"content\": \"Test Lambda functions locally using SAM CLI, implement unit tests, integration tests, and use AWS X-Ray for debugging distributed applications.\"},\n",
    "    {\"id\": \"lambda_scaling\", \"title\": \"Lambda Scaling\", \"content\": \"Lambda automatically scales from zero to thousands of concurrent executions. Understand scaling limits and configure reserved concurrency as needed.\"},\n",
    "    {\"id\": \"lambda_integration\", \"title\": \"Lambda Integration Patterns\", \"content\": \"Integrate Lambda with other AWS services using event-driven patterns, API Gateway for REST APIs, and Step Functions for workflows.\"},\n",
    "    {\"id\": \"lambda_troubleshooting\", \"title\": \"Lambda Troubleshooting\", \"content\": \"Troubleshoot Lambda issues using CloudWatch Logs, X-Ray tracing, and monitoring key metrics like duration, errors, and throttles.\"}\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents for re-ranking demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Get embedding using Titan model\"\"\"\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=EMBEDDING_MODEL,\n",
    "        body=json.dumps({\"inputText\": text})\n",
    "    )\n",
    "    return json.loads(response['body'].read())['embedding']\n",
    "\n",
    "# Create vector index\n",
    "print(\"Creating vector embeddings...\")\n",
    "vector_index = {}\n",
    "\n",
    "for doc in documents:\n",
    "    embedding = get_embedding(doc[\"content\"])\n",
    "    vector_index[doc[\"id\"]] = {\n",
    "        \"embedding\": embedding,\n",
    "        \"document\": doc\n",
    "    }\n",
    "    print(f\"Embedded {doc['id']}\")\n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(\"Vector index created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a: List[float], b: List[float]) -> float:\n",
    "    \"\"\"Calculate cosine similarity\"\"\"\n",
    "    a_np = np.array(a)\n",
    "    b_np = np.array(b)\n",
    "    return np.dot(a_np, b_np) / (np.linalg.norm(a_np) * np.linalg.norm(b_np))\n",
    "\n",
    "def initial_retrieval(query: str, top_k: int = 20) -> List[Dict]:\n",
    "    \"\"\"Stage 1: Initial retrieval of top candidates\"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    similarities = []\n",
    "    for doc_id, data in vector_index.items():\n",
    "        similarity = cosine_similarity(query_embedding, data[\"embedding\"])\n",
    "        similarities.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"document\": data[\"document\"],\n",
    "            \"initial_score\": similarity\n",
    "        })\n",
    "    \n",
    "    # Sort by similarity and return top_k\n",
    "    similarities.sort(key=lambda x: x[\"initial_score\"], reverse=True)\n",
    "    return similarities[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_with_llm(query: str, candidates: List[Dict], top_k: int = 5) -> List[Dict]:\n",
    "    \"\"\"Stage 2: Re-rank candidates using LLM for relevance scoring\"\"\"\n",
    "    \n",
    "    # Prepare candidates for re-ranking\n",
    "    candidate_texts = []\n",
    "    for i, candidate in enumerate(candidates):\n",
    "        doc = candidate[\"document\"]\n",
    "        candidate_texts.append(f\"Document {i+1}: {doc['title']}\\n{doc['content']}\")\n",
    "    \n",
    "    # Create re-ranking prompt\n",
    "    candidates_text = \"\\n\\n\".join(candidate_texts)\n",
    "    \n",
    "    rerank_prompt = f\"\"\"You are a relevance scoring system. Given a query and a list of documents, score each document's relevance to the query on a scale of 0-100.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Documents:\n",
    "{candidates_text}\n",
    "\n",
    "Provide relevance scores in JSON format:\n",
    "{{\"scores\": [score1, score2, score3, ...]}}\n",
    "\n",
    "Consider:\n",
    "- Direct relevance to the query\n",
    "- Completeness of information\n",
    "- Specificity to the question asked\n",
    "\n",
    "JSON Response:\"\"\"\n",
    "    \n",
    "    # Get re-ranking scores from Nova\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=GENERATION_MODEL,\n",
    "        body=json.dumps({\n",
    "            \"messages\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"text\": rerank_prompt}]\n",
    "            }],\n",
    "            \"inferenceConfig\": {\n",
    "                \"maxTokens\": 500,\n",
    "                \"temperature\": 0.1\n",
    "            }\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response['body'].read())\n",
    "    rerank_response = result['output']['message']['content'][0]['text']\n",
    "    \n",
    "    try:\n",
    "        # Extract JSON from response\n",
    "        json_match = re.search(r'\\{.*\\}', rerank_response, re.DOTALL)\n",
    "        if json_match:\n",
    "            scores_data = json.loads(json_match.group())\n",
    "            scores = scores_data.get(\"scores\", [])\n",
    "        else:\n",
    "            # Fallback: use initial scores\n",
    "            scores = [candidate[\"initial_score\"] * 100 for candidate in candidates]\n",
    "    except:\n",
    "        # Fallback: use initial scores\n",
    "        scores = [candidate[\"initial_score\"] * 100 for candidate in candidates]\n",
    "    \n",
    "    # Add re-ranking scores to candidates\n",
    "    for i, candidate in enumerate(candidates):\n",
    "        if i < len(scores):\n",
    "            candidate[\"rerank_score\"] = scores[i]\n",
    "        else:\n",
    "            candidate[\"rerank_score\"] = candidate[\"initial_score\"] * 100\n",
    "    \n",
    "    # Sort by re-ranking score and return top_k\n",
    "    candidates.sort(key=lambda x: x[\"rerank_score\"], reverse=True)\n",
    "    return candidates[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_with_reranked_context(query: str, reranked_docs: List[Dict]) -> str:\n",
    "    \"\"\"Stage 3: Generate answer using high-quality re-ranked context\"\"\"\n",
    "    \n",
    "    context_parts = []\n",
    "    for doc_data in reranked_docs:\n",
    "        doc = doc_data[\"document\"]\n",
    "        score = doc_data[\"rerank_score\"]\n",
    "        context_parts.append(f\"[Relevance: {score:.1f}] {doc['title']}: {doc['content']}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    prompt = f\"\"\"Based on the following high-quality, re-ranked context, provide a comprehensive and accurate answer.\n",
    "\n",
    "Context (with relevance scores):\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=GENERATION_MODEL,\n",
    "        body=json.dumps({\n",
    "            \"messages\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"text\": prompt}]\n",
    "            }],\n",
    "            \"inferenceConfig\": {\n",
    "                \"maxTokens\": 400,\n",
    "                \"temperature\": 0.1\n",
    "            }\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response['body'].read())\n",
    "    return result['output']['message']['content'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reranking_rag_pipeline(query: str) -> Dict:\n",
    "    \"\"\"Complete re-ranking RAG pipeline\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RE-RANKING RAG PIPELINE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    # Stage 1: Initial retrieval (top 20)\n",
    "    print(\"Stage 1: Initial Retrieval (top 20)\")\n",
    "    print(\"-\" * 40)\n",
    "    initial_candidates = initial_retrieval(query, top_k=20)\n",
    "    \n",
    "    print(\"Top 5 initial candidates:\")\n",
    "    for i, candidate in enumerate(initial_candidates[:5], 1):\n",
    "        doc = candidate[\"document\"]\n",
    "        score = candidate[\"initial_score\"]\n",
    "        print(f\"  {i}. {doc['title']} (similarity: {score:.3f})\")\n",
    "    \n",
    "    print(f\"\\nRetrieved {len(initial_candidates)} candidates for re-ranking\\n\")\n",
    "    \n",
    "    # Stage 2: Re-ranking (top 5)\n",
    "    print(\"Stage 2: Re-ranking (top 5)\")\n",
    "    print(\"-\" * 40)\n",
    "    reranked_candidates = rerank_with_llm(query, initial_candidates, top_k=5)\n",
    "    \n",
    "    print(\"Re-ranked results:\")\n",
    "    for i, candidate in enumerate(reranked_candidates, 1):\n",
    "        doc = candidate[\"document\"]\n",
    "        initial_score = candidate[\"initial_score\"]\n",
    "        rerank_score = candidate[\"rerank_score\"]\n",
    "        print(f\"  {i}. {doc['title']}\")\n",
    "        print(f\"     Initial: {initial_score:.3f} → Re-ranked: {rerank_score:.1f}\")\n",
    "    \n",
    "    # Stage 3: Generation\n",
    "    print(f\"\\nStage 3: Answer Generation\")\n",
    "    print(\"-\" * 40)\n",
    "    answer = generate_answer_with_reranked_context(query, reranked_candidates)\n",
    "    \n",
    "    print(f\"Final Answer: {answer}\")\n",
    "    print(f\"\\n{'='*60}\\n\")\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"initial_candidates\": len(initial_candidates),\n",
    "        \"reranked_candidates\": len(reranked_candidates),\n",
    "        \"final_sources\": [c[\"document\"][\"title\"] for c in reranked_candidates],\n",
    "        \"rerank_scores\": [c[\"rerank_score\"] for c in reranked_candidates],\n",
    "        \"answer\": answer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test re-ranking RAG pipeline\n",
    "test_questions = [\n",
    "    \"How can I optimize Lambda costs?\",\n",
    "    \"What are Lambda cold start issues and solutions?\",\n",
    "    \"How to monitor Lambda function performance?\",\n",
    "    \"Lambda security best practices?\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "for question in test_questions:\n",
    "    result = reranking_rag_pipeline(question)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with standard RAG (no re-ranking)\n",
    "def standard_rag(query: str, top_k: int = 5) -> Dict:\n",
    "    \"\"\"Standard RAG without re-ranking for comparison\"\"\"\n",
    "    \n",
    "    # Direct retrieval without re-ranking\n",
    "    candidates = initial_retrieval(query, top_k=top_k)\n",
    "    \n",
    "    # Generate answer directly\n",
    "    context_parts = []\n",
    "    for candidate in candidates:\n",
    "        doc = candidate[\"document\"]\n",
    "        context_parts.append(f\"{doc['title']}: {doc['content']}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    prompt = f\"\"\"Based on the following context, answer the question:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=GENERATION_MODEL,\n",
    "        body=json.dumps({\n",
    "            \"messages\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"text\": prompt}]\n",
    "            }],\n",
    "            \"inferenceConfig\": {\n",
    "                \"maxTokens\": 400,\n",
    "                \"temperature\": 0.1\n",
    "            }\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response['body'].read())\n",
    "    answer = result['output']['message']['content'][0]['text']\n",
    "    \n",
    "    return {\n",
    "        \"sources\": [c[\"document\"][\"title\"] for c in candidates],\n",
    "        \"scores\": [c[\"initial_score\"] for c in candidates],\n",
    "        \"answer\": answer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison: Re-ranking vs Standard RAG\n",
    "print(\"COMPARISON: RE-RANKING vs STANDARD RAG\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "sample_query = \"How to reduce Lambda costs?\"\n",
    "print(f\"Query: {sample_query}\\n\")\n",
    "\n",
    "# Standard RAG\n",
    "print(\"Standard RAG (no re-ranking):\")\n",
    "standard_result = standard_rag(sample_query)\n",
    "print(\"Sources:\")\n",
    "for i, (source, score) in enumerate(zip(standard_result[\"sources\"], standard_result[\"scores\"]), 1):\n",
    "    print(f\"  {i}. {source} (similarity: {score:.3f})\")\n",
    "print(f\"\\nAnswer: {standard_result['answer'][:150]}...\\n\")\n",
    "\n",
    "# Re-ranking RAG\n",
    "print(\"Re-ranking RAG:\")\n",
    "rerank_result = reranking_rag_pipeline(sample_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-ranking Pipeline Benefits\n",
    "\n",
    "### Standard RAG Limitations:\n",
    "❌ **Single-stage retrieval** may miss nuanced relevance  \n",
    "❌ **Vector similarity alone** doesn't capture query intent perfectly  \n",
    "❌ **No quality refinement** of retrieved context  \n",
    "\n",
    "### Re-ranking Pipeline Advantages:\n",
    "✅ **Two-stage refinement**: Broad retrieval → Precise re-ranking  \n",
    "✅ **LLM-based relevance scoring**: Better understanding of query intent  \n",
    "✅ **Quality-focused context**: Only the most relevant documents for generation  \n",
    "✅ **Improved answer quality**: Higher precision with focused context  \n",
    "\n",
    "### Pipeline Stages:\n",
    "1. **Initial Retrieval (20 candidates)**: Cast wide net with vector similarity\n",
    "2. **Re-ranking (5 best)**: LLM evaluates true relevance to query\n",
    "3. **Generation**: High-quality context produces better answers\n",
    "\n",
    "### When to Use Re-ranking:\n",
    "- **High-quality requirements**: When answer accuracy is critical\n",
    "- **Complex queries**: Multi-faceted questions needing precise context\n",
    "- **Large document collections**: When initial retrieval may be noisy\n",
    "- **Domain expertise**: When semantic similarity isn't enough\n",
    "\n",
    "### Trade-offs:\n",
    "- **Higher latency**: Additional LLM call for re-ranking\n",
    "- **Increased cost**: More model invocations\n",
    "- **Better quality**: Significantly improved answer relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance analysis\n",
    "print(\"RE-RANKING PIPELINE PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "total_rerank_score = 0\n",
    "for result in results:\n",
    "    avg_rerank_score = sum(result[\"rerank_scores\"]) / len(result[\"rerank_scores\"])\n",
    "    total_rerank_score += avg_rerank_score\n",
    "    \n",
    "    print(f\"Query: {result['query'][:40]}...\")\n",
    "    print(f\"  Initial candidates: {result['initial_candidates']}\")\n",
    "    print(f\"  Final candidates: {result['reranked_candidates']}\")\n",
    "    print(f\"  Avg re-rank score: {avg_rerank_score:.1f}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Overall average re-rank score: {total_rerank_score/len(results):.1f}\")\n",
    "print(f\"\\nDemo complete! Re-ranking bucket: {RERANK_BUCKET}\")\n",
    "print(\"Re-ranking pipeline provides superior answer quality through two-stage refinement.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
