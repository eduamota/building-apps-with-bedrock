{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 Vectors Hybrid Search Demo\n",
    "**Pattern:** Hybrid search combining S3 Vectors (semantic) + keyword search (lexical)\n",
    "\n",
    "**Components:**\n",
    "- Amazon S3 Vectors for semantic search\n",
    "- BM25 keyword search\n",
    "- Reciprocal Rank Fusion (RRF)\n",
    "- Amazon Bedrock for embeddings and generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import Counter\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize clients\n",
    "bedrock_runtime = boto3.client('bedrock-runtime')\n",
    "s3_vectors = boto3.client('s3vectors')\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Configuration\n",
    "VECTOR_BUCKET = f\"hybrid-vectors-{int(time.time())}\"\n",
    "INDEX_NAME = \"documents\"\n",
    "EMBEDDING_MODEL = \"amazon.titan-embed-text-v2:0\"\n",
    "GENERATION_MODEL = \"amazon.nova-pro-v1:0\"\n",
    "REGION = \"us-east-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup S3 Vector Bucket and Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created vector bucket: hybrid-vectors-1767740568\n",
      "Created index: documents\n"
     ]
    }
   ],
   "source": [
    "def create_vector_infrastructure():\n",
    "    \"\"\"Create S3 vector bucket and index\"\"\"\n",
    "    try:\n",
    "        # Create vector bucket\n",
    "        s3_vectors.create_vector_bucket(\n",
    "            vectorBucketName=VECTOR_BUCKET\n",
    "        )\n",
    "        print(f\"Created vector bucket: {VECTOR_BUCKET}\")\n",
    "        \n",
    "        # Create vector index\n",
    "        s3_vectors.create_index(\n",
    "            vectorBucketName=VECTOR_BUCKET,\n",
    "            indexName=INDEX_NAME,\n",
    "            dimension=1024,  # Titan v2 dimensions\n",
    "            dataType=\"float32\",\n",
    "            distanceMetric=\"cosine\",\n",
    "            metadataConfiguration={\"nonFilterableMetadataKeys\": [\"content\", \"source\"]}\n",
    "        )\n",
    "        print(f\"Created index: {INDEX_NAME}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Setup error: {e}\")\n",
    "\n",
    "create_vector_infrastructure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document Processing and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Generate embedding using Bedrock\"\"\"\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=EMBEDDING_MODEL,\n",
    "        body=json.dumps({\"inputText\": text})\n",
    "    )\n",
    "    return json.loads(response['body'].read())['embedding']\n",
    "\n",
    "def chunk_document(text: str, chunk_size: int = 500) -> List[str]:\n",
    "    \"\"\"Simple text chunking\"\"\"\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 5 vectors\n"
     ]
    }
   ],
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "    \"Amazon S3 Vectors provides cost-effective vector storage for AI applications with sub-second query performance.\",\n",
    "    \"Machine learning models require vector embeddings to represent text data in high-dimensional space.\",\n",
    "    \"Retrieval Augmented Generation combines vector search with large language models for better responses.\",\n",
    "    \"Hybrid search merges semantic similarity with keyword matching for comprehensive document retrieval.\",\n",
    "    \"Amazon Bedrock offers foundation models for embedding generation and text completion tasks.\"\n",
    "]\n",
    "\n",
    "# Process and store documents\n",
    "def store_documents():\n",
    "    vectors_to_insert = []\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        embedding = get_embedding(doc)\n",
    "        \n",
    "        vector_data = {\n",
    "            \"key\": f\"doc_{i}\",\n",
    "            \"data\": {\"float32\": embedding},\n",
    "            \"metadata\": {\n",
    "                \"content\": doc,\n",
    "                \"source\": f\"document_{i}\",\n",
    "                \"type\": \"text\"\n",
    "            }\n",
    "        }\n",
    "        vectors_to_insert.append(vector_data)\n",
    "    \n",
    "    # Insert vectors into S3 Vectors\n",
    "    s3_vectors.put_vectors(\n",
    "        vectorBucketName=VECTOR_BUCKET,\n",
    "        indexName=INDEX_NAME,\n",
    "        vectors=vectors_to_insert\n",
    "    )\n",
    "    print(f\"Stored {len(vectors_to_insert)} vectors\")\n",
    "    return documents\n",
    "\n",
    "stored_docs = store_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BM25 Keyword Search Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 index created\n"
     ]
    }
   ],
   "source": [
    "class BM25:\n",
    "    def __init__(self, documents: List[str], k1=1.5, b=0.75):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.documents = documents\n",
    "        self.doc_count = len(documents)\n",
    "        \n",
    "        # Tokenize and build index\n",
    "        self.tokenized_docs = [self._tokenize(doc) for doc in documents]\n",
    "        self.doc_lengths = [len(doc) for doc in self.tokenized_docs]\n",
    "        self.avg_doc_length = sum(self.doc_lengths) / self.doc_count\n",
    "        \n",
    "        # Build term frequency and document frequency\n",
    "        self.term_frequencies = []\n",
    "        self.document_frequencies = Counter()\n",
    "        \n",
    "        for doc in self.tokenized_docs:\n",
    "            tf = Counter(doc)\n",
    "            self.term_frequencies.append(tf)\n",
    "            for term in tf.keys():\n",
    "                self.document_frequencies[term] += 1\n",
    "    \n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 5) -> List[Tuple[int, float]]:\n",
    "        query_terms = self._tokenize(query)\n",
    "        scores = []\n",
    "        \n",
    "        for doc_idx in range(self.doc_count):\n",
    "            score = 0\n",
    "            doc_length = self.doc_lengths[doc_idx]\n",
    "            \n",
    "            for term in query_terms:\n",
    "                if term in self.term_frequencies[doc_idx]:\n",
    "                    tf = self.term_frequencies[doc_idx][term]\n",
    "                    df = self.document_frequencies[term]\n",
    "                    idf = math.log((self.doc_count - df + 0.5) / (df + 0.5))\n",
    "                    \n",
    "                    score += idf * (tf * (self.k1 + 1)) / (\n",
    "                        tf + self.k1 * (1 - self.b + self.b * doc_length / self.avg_doc_length)\n",
    "                    )\n",
    "            \n",
    "            scores.append((doc_idx, score))\n",
    "        \n",
    "        return sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "# Initialize BM25\n",
    "bm25 = BM25(stored_docs)\n",
    "print(\"BM25 index created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hybrid Search Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(query: str, top_k: int = 5) -> List[Tuple[str, float, str]]:\n",
    "    \"\"\"Semantic search using S3 Vectors\"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    response = s3_vectors.query_vectors(\n",
    "        vectorBucketName=VECTOR_BUCKET,\n",
    "        indexName=INDEX_NAME,\n",
    "        queryVector={\"float32\": query_embedding},\n",
    "        topK=top_k,\n",
    "        returnDistance=True,\n",
    "        returnMetadata=True\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    for result in response['vectors']:\n",
    "        key = result['key']\n",
    "        distance = result['distance']\n",
    "        content = result['metadata']['content']\n",
    "        similarity = 1 - distance  # Convert distance to similarity\n",
    "        results.append((key, similarity, content))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def reciprocal_rank_fusion(vector_results: List, bm25_results: List, k: int = 60) -> List[Tuple[str, float]]:\n",
    "    \"\"\"Combine rankings using RRF\"\"\"\n",
    "    scores = {}\n",
    "    \n",
    "    # Process vector results\n",
    "    for rank, (doc_key, similarity, content) in enumerate(vector_results):\n",
    "        doc_idx = int(doc_key.split('_')[1])\n",
    "        scores[doc_idx] = scores.get(doc_idx, 0) + 1 / (k + rank + 1)\n",
    "    \n",
    "    # Process BM25 results\n",
    "    for rank, (doc_idx, bm25_score) in enumerate(bm25_results):\n",
    "        scores[doc_idx] = scores.get(doc_idx, 0) + 1 / (k + rank + 1)\n",
    "    \n",
    "    # Sort by combined score\n",
    "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "def hybrid_search(query: str, top_k: int = 3) -> List[Dict]:\n",
    "    \"\"\"Perform hybrid search combining vector and keyword search\"\"\"\n",
    "    # Get results from both methods\n",
    "    vector_results = vector_search(query, top_k)\n",
    "    bm25_results = bm25.search(query, top_k)\n",
    "    \n",
    "    print(f\"Vector results: {[(r[0], f'{r[1]:.3f}') for r in vector_results]}\")\n",
    "    print(f\"BM25 results: {[(r[0], f'{r[1]:.3f}') for r in bm25_results]}\")\n",
    "    \n",
    "    # Combine using RRF\n",
    "    fused_results = reciprocal_rank_fusion(vector_results, bm25_results)\n",
    "    \n",
    "    # Format final results\n",
    "    final_results = []\n",
    "    for doc_idx, rrf_score in fused_results[:top_k]:\n",
    "        final_results.append({\n",
    "            'document': stored_docs[doc_idx],\n",
    "            'doc_id': doc_idx,\n",
    "            'rrf_score': rrf_score\n",
    "        })\n",
    "    \n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Query: 'vector storage cost effective' ===\n",
      "Vector results: [('doc_0', '0.535'), ('doc_1', '0.238'), ('doc_2', '0.152')]\n",
      "BM25 results: [(0, '2.722'), (3, '0.000'), (4, '0.000')]\n",
      "1. [Score: 0.0328] Amazon S3 Vectors provides cost-effective vector storage for AI applications with sub-second query p...\n",
      "2. [Score: 0.0161] Machine learning models require vector embeddings to represent text data in high-dimensional space....\n",
      "3. [Score: 0.0161] Hybrid search merges semantic similarity with keyword matching for comprehensive document retrieval....\n",
      "\n",
      "=== Query: 'machine learning embeddings' ===\n",
      "Vector results: [('doc_1', '0.644'), ('doc_4', '0.303'), ('doc_2', '0.196')]\n",
      "BM25 results: [(1, '3.231'), (0, '0.000'), (2, '0.000')]\n",
      "1. [Score: 0.0328] Machine learning models require vector embeddings to represent text data in high-dimensional space....\n",
      "2. [Score: 0.0317] Retrieval Augmented Generation combines vector search with large language models for better response...\n",
      "3. [Score: 0.0161] Amazon Bedrock offers foundation models for embedding generation and text completion tasks....\n",
      "\n",
      "=== Query: 'hybrid search semantic keyword' ===\n",
      "Vector results: [('doc_3', '0.791'), ('doc_2', '0.141'), ('doc_1', '0.121')]\n",
      "BM25 results: [(3, '3.812'), (2, '0.341'), (0, '0.000')]\n",
      "1. [Score: 0.0328] Hybrid search merges semantic similarity with keyword matching for comprehensive document retrieval....\n",
      "2. [Score: 0.0323] Retrieval Augmented Generation combines vector search with large language models for better response...\n",
      "3. [Score: 0.0159] Machine learning models require vector embeddings to represent text data in high-dimensional space....\n"
     ]
    }
   ],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"vector storage cost effective\",\n",
    "    \"machine learning embeddings\",\n",
    "    \"hybrid search semantic keyword\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n=== Query: '{query}' ===\")\n",
    "    results = hybrid_search(query)\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"{i}. [Score: {result['rrf_score']:.4f}] {result['document'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RAG with Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query: str, context_docs: List[str]) -> str:\n",
    "    \"\"\"Generate response using Bedrock\"\"\"\n",
    "    context = \"\\n\\n\".join([f\"Document {i+1}: {doc}\" for i, doc in enumerate(context_docs)])\n",
    "    \n",
    "    prompt = f\"\"\"Based on the following context, answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=GENERATION_MODEL,\n",
    "        body=json.dumps({\n",
    "            \"schemaVersion\": \"messages-v1\",\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}],\n",
    "            \"inferenceConfig\": {\"maxTokens\": 500, \"temperature\": 0.1}\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    return json.loads(response['body'].read())['output']['message']['content'][0]['text']\n",
    "\n",
    "def hybrid_rag(query: str) -> str:\n",
    "    \"\"\"Complete RAG pipeline with hybrid search\"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    search_results = hybrid_search(query)\n",
    "    context_docs = [result['document'] for result in search_results]\n",
    "    \n",
    "    # Generate response\n",
    "    response = generate_response(query, context_docs)\n",
    "    \n",
    "    return response, search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector results: [('doc_0', '0.803'), ('doc_1', '0.316'), ('doc_4', '0.242')]\n",
      "BM25 results: [(0, '5.753'), (1, '0.000'), (4, '0.000')]\n",
      "Question: How does S3 Vectors help with cost-effective AI applications?\n",
      "\n",
      "Answer: Amazon S3 Vectors helps with cost-effective AI applications by providing a specialized storage solution designed for vector data, which is commonly used in machine learning models. Hereâ€™s how it achieves cost-effectiveness:\n",
      "\n",
      "1. **Efficient Storage**: S3 Vectors is optimized for storing vector embeddings, which are essential for representing text data in high-dimensional space as mentioned in Document 2. This specialized storage likely reduces the overhead and inefficiency associated with storing such data in general-purpose storage solutions.\n",
      "\n",
      "2. **Sub-second Query Performance**: As highlighted in Document 1, S3 Vectors offers sub-second query performance. This rapid access to vector data means that AI applications can retrieve the necessary embeddings quickly, reducing latency and improving overall application performance. Faster data retrieval can lead to more efficient use of compute resources, thereby lowering costs.\n",
      "\n",
      "3. **Integration with AI Services**: By leveraging Amazon S3 Vectors alongside other AWS AI services like Amazon Bedrock (as mentioned in Document 3), users can create a streamlined AI workflow. Bedrock provides foundation models for tasks like embedding generation, and S3 Vectors stores these embeddings efficiently. This integration minimizes the need for additional infrastructure and simplifies the deployment of AI applications, contributing to cost savings.\n",
      "\n",
      "In summary, Amazon S3 Vectors helps achieve cost-effectiveness in AI applications through efficient vector storage, rapid data access, and seamless integration with other AWS AI services.\n",
      "\n",
      "Sources used:\n",
      "1. [RRF: 0.0328] Amazon S3 Vectors provides cost-effective vector storage for AI applications wit...\n",
      "2. [RRF: 0.0323] Machine learning models require vector embeddings to represent text data in high...\n",
      "3. [RRF: 0.0317] Amazon Bedrock offers foundation models for embedding generation and text comple...\n"
     ]
    }
   ],
   "source": [
    "# Test RAG pipeline\n",
    "question = \"How does S3 Vectors help with cost-effective AI applications?\"\n",
    "\n",
    "answer, sources = hybrid_rag(question)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"\\nAnswer: {answer}\")\n",
    "print(f\"\\nSources used:\")\n",
    "for i, source in enumerate(sources, 1):\n",
    "    print(f\"{i}. [RRF: {source['rrf_score']:.4f}] {source['document'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted index: documents\n",
      "Deleted vector bucket: hybrid-vectors-1767740568\n"
     ]
    }
   ],
   "source": [
    "def cleanup():\n",
    "    \"\"\"Clean up S3 Vectors resources\"\"\"\n",
    "    try:\n",
    "        # Delete index\n",
    "        s3_vectors.delete_index(\n",
    "            vectorBucketName=VECTOR_BUCKET,\n",
    "            indexName=INDEX_NAME\n",
    "        )\n",
    "        print(f\"Deleted index: {INDEX_NAME}\")\n",
    "        \n",
    "        # Delete vector bucket\n",
    "        s3_vectors.delete_vector_bucket(\n",
    "            vectorBucketName=VECTOR_BUCKET\n",
    "        )\n",
    "        print(f\"Deleted vector bucket: {VECTOR_BUCKET}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Cleanup error: {e}\")\n",
    "\n",
    "# Uncomment to cleanup\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
