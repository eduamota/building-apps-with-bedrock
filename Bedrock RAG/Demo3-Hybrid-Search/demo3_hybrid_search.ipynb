{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 3: Hybrid Search RAG (Vector + Keyword)\n",
    "Pattern: Advanced Retrieval with Dual Search\n",
    "\n",
    "**Components:**\n",
    "- Vector similarity search (semantic)\n",
    "- Keyword/BM25 search (lexical)\n",
    "- Reciprocal Rank Fusion (RRF) merging\n",
    "- Query transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize clients\n",
    "bedrock_runtime = boto3.client('bedrock-runtime')\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Configuration\n",
    "HYBRID_BUCKET = f\"hybrid-search-demo-{int(time.time())}\"\n",
    "EMBEDDING_MODEL = \"amazon.titan-embed-text-v1\"\n",
    "GENERATION_MODEL = \"amazon.nova-pro-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create S3 bucket\n",
    "s3.create_bucket(Bucket=HYBRID_BUCKET)\n",
    "print(f\"Created hybrid search bucket: {HYBRID_BUCKET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended document collection for hybrid search\n",
    "documents = [\n",
    "    {\n",
    "        \"id\": \"lambda_pricing\",\n",
    "        \"title\": \"AWS Lambda Pricing Model\",\n",
    "        \"content\": \"AWS Lambda pricing is based on requests and compute time. You pay $0.20 per 1M requests and $0.0000166667 per GB-second. Free tier includes 1M requests monthly.\",\n",
    "        \"keywords\": [\"pricing\", \"cost\", \"billing\", \"free tier\", \"requests\", \"compute\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lambda_memory\",\n",
    "        \"title\": \"Lambda Memory Configuration\",\n",
    "        \"content\": \"Configure Lambda memory from 128 MB to 10,240 MB. CPU power scales with memory allocation. Higher memory improves performance but increases cost per execution.\",\n",
    "        \"keywords\": [\"memory\", \"configuration\", \"CPU\", \"performance\", \"scaling\", \"allocation\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lambda_timeout\",\n",
    "        \"title\": \"Lambda Timeout Settings\",\n",
    "        \"content\": \"Lambda maximum execution time is 15 minutes (900 seconds). Default timeout is 3 seconds. Configure timeout based on function requirements plus buffer time.\",\n",
    "        \"keywords\": [\"timeout\", \"execution\", \"duration\", \"limits\", \"configuration\", \"seconds\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lambda_coldstart\",\n",
    "        \"title\": \"Lambda Cold Start Optimization\",\n",
    "        \"content\": \"Cold starts add latency when Lambda initializes new execution environments. Use provisioned concurrency, optimize package size, and minimize initialization code to reduce cold starts.\",\n",
    "        \"keywords\": [\"cold start\", \"latency\", \"initialization\", \"provisioned concurrency\", \"optimization\", \"performance\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lambda_vpc\",\n",
    "        \"title\": \"Lambda VPC Configuration\",\n",
    "        \"content\": \"Lambda functions can access VPC resources like RDS databases and private subnets. VPC configuration adds cold start latency. Use VPC endpoints for AWS services.\",\n",
    "        \"keywords\": [\"VPC\", \"networking\", \"RDS\", \"subnets\", \"endpoints\", \"private\", \"security\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lambda_monitoring\",\n",
    "        \"title\": \"Lambda Monitoring and Logging\",\n",
    "        \"content\": \"Monitor Lambda with CloudWatch metrics: Duration, Invocations, Errors, Throttles. Enable X-Ray tracing for distributed systems. Use structured logging for better analysis.\",\n",
    "        \"keywords\": [\"monitoring\", \"CloudWatch\", \"metrics\", \"X-Ray\", \"tracing\", \"logging\", \"errors\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lambda_security\",\n",
    "        \"title\": \"Lambda Security Best Practices\",\n",
    "        \"content\": \"Use IAM roles with least privilege. Store secrets in AWS Secrets Manager. Enable encryption at rest and in transit. Validate input data to prevent injection attacks.\",\n",
    "        \"keywords\": [\"security\", \"IAM\", \"secrets\", \"encryption\", \"validation\", \"least privilege\", \"injection\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lambda_deployment\",\n",
    "        \"title\": \"Lambda Deployment Strategies\",\n",
    "        \"content\": \"Deploy Lambda using blue/green, canary, or all-at-once strategies. Use AWS CodeDeploy for automated deployments. Implement proper testing and rollback procedures.\",\n",
    "        \"keywords\": [\"deployment\", \"blue/green\", \"canary\", \"CodeDeploy\", \"testing\", \"rollback\", \"automation\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents for hybrid search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Get embedding using Titan model\"\"\"\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=EMBEDDING_MODEL,\n",
    "        body=json.dumps({\"inputText\": text})\n",
    "    )\n",
    "    return json.loads(response['body'].read())['embedding']\n",
    "\n",
    "def preprocess_text(text: str) -> List[str]:\n",
    "    \"\"\"Preprocess text for keyword search\"\"\"\n",
    "    # Convert to lowercase and split into tokens\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    # Remove common stop words\n",
    "    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were'}\n",
    "    return [token for token in tokens if token not in stop_words and len(token) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hybrid index (vector + keyword)\n",
    "print(\"Creating hybrid index...\")\n",
    "\n",
    "hybrid_index = {\n",
    "    \"documents\": {},\n",
    "    \"vectors\": {},\n",
    "    \"keyword_index\": {},\n",
    "    \"doc_frequencies\": {},\n",
    "    \"total_docs\": len(documents)\n",
    "}\n",
    "\n",
    "# Build inverted index for keyword search\n",
    "for doc in documents:\n",
    "    doc_id = doc[\"id\"]\n",
    "    content = doc[\"content\"]\n",
    "    \n",
    "    # Store document\n",
    "    hybrid_index[\"documents\"][doc_id] = doc\n",
    "    \n",
    "    # Create vector embedding\n",
    "    embedding = get_embedding(content)\n",
    "    hybrid_index[\"vectors\"][doc_id] = embedding\n",
    "    \n",
    "    # Process text for keyword search\n",
    "    tokens = preprocess_text(content + \" \" + \" \".join(doc[\"keywords\"]))\n",
    "    token_counts = Counter(tokens)\n",
    "    \n",
    "    # Build inverted index\n",
    "    for token, count in token_counts.items():\n",
    "        if token not in hybrid_index[\"keyword_index\"]:\n",
    "            hybrid_index[\"keyword_index\"][token] = {}\n",
    "        hybrid_index[\"keyword_index\"][token][doc_id] = count\n",
    "        \n",
    "        # Track document frequency\n",
    "        if token not in hybrid_index[\"doc_frequencies\"]:\n",
    "            hybrid_index[\"doc_frequencies\"][token] = 0\n",
    "        hybrid_index[\"doc_frequencies\"][token] += 1\n",
    "    \n",
    "    print(f\"Indexed {doc_id}\")\n",
    "    time.sleep(0.1)\n",
    "\n",
    "# Store index in S3\n",
    "s3.put_object(\n",
    "    Bucket=HYBRID_BUCKET,\n",
    "    Key=\"hybrid_index.json\",\n",
    "    Body=json.dumps(hybrid_index)\n",
    ")\n",
    "\n",
    "print(\"Hybrid index created and stored in S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a: List[float], b: List[float]) -> float:\n",
    "    \"\"\"Calculate cosine similarity\"\"\"\n",
    "    a_np = np.array(a)\n",
    "    b_np = np.array(b)\n",
    "    return np.dot(a_np, b_np) / (np.linalg.norm(a_np) * np.linalg.norm(b_np))\n",
    "\n",
    "def vector_search(query: str, top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "    \"\"\"Perform vector similarity search\"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    similarities = []\n",
    "    for doc_id, doc_embedding in hybrid_index[\"vectors\"].items():\n",
    "        similarity = cosine_similarity(query_embedding, doc_embedding)\n",
    "        similarities.append((doc_id, similarity))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "def bm25_score(query_tokens: List[str], doc_id: str, k1: float = 1.5, b: float = 0.75) -> float:\n",
    "    \"\"\"Calculate BM25 score for document\"\"\"\n",
    "    doc = hybrid_index[\"documents\"][doc_id]\n",
    "    doc_tokens = preprocess_text(doc[\"content\"] + \" \" + \" \".join(doc[\"keywords\"]))\n",
    "    doc_length = len(doc_tokens)\n",
    "    \n",
    "    # Average document length\n",
    "    avg_doc_length = sum(len(preprocess_text(d[\"content\"])) for d in hybrid_index[\"documents\"].values()) / len(hybrid_index[\"documents\"])\n",
    "    \n",
    "    score = 0.0\n",
    "    doc_token_counts = Counter(doc_tokens)\n",
    "    \n",
    "    for token in query_tokens:\n",
    "        if token in hybrid_index[\"keyword_index\"] and doc_id in hybrid_index[\"keyword_index\"][token]:\n",
    "            tf = doc_token_counts[token]  # Term frequency\n",
    "            df = hybrid_index[\"doc_frequencies\"][token]  # Document frequency\n",
    "            idf = math.log((hybrid_index[\"total_docs\"] - df + 0.5) / (df + 0.5))  # Inverse document frequency\n",
    "            \n",
    "            # BM25 formula\n",
    "            numerator = tf * (k1 + 1)\n",
    "            denominator = tf + k1 * (1 - b + b * (doc_length / avg_doc_length))\n",
    "            score += idf * (numerator / denominator)\n",
    "    \n",
    "    return score\n",
    "\n",
    "def keyword_search(query: str, top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "    \"\"\"Perform BM25 keyword search\"\"\"\n",
    "    query_tokens = preprocess_text(query)\n",
    "    \n",
    "    scores = []\n",
    "    for doc_id in hybrid_index[\"documents\"].keys():\n",
    "        score = bm25_score(query_tokens, doc_id)\n",
    "        scores.append((doc_id, score))\n",
    "    \n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scores[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(vector_results: List[Tuple[str, float]], \n",
    "                          keyword_results: List[Tuple[str, float]], \n",
    "                          k: int = 60) -> List[Tuple[str, float]]:\n",
    "    \"\"\"Combine results using Reciprocal Rank Fusion\"\"\"\n",
    "    \n",
    "    # Create rank mappings\n",
    "    vector_ranks = {doc_id: rank + 1 for rank, (doc_id, _) in enumerate(vector_results)}\n",
    "    keyword_ranks = {doc_id: rank + 1 for rank, (doc_id, _) in enumerate(keyword_results)}\n",
    "    \n",
    "    # Get all unique documents\n",
    "    all_docs = set(vector_ranks.keys()) | set(keyword_ranks.keys())\n",
    "    \n",
    "    # Calculate RRF scores\n",
    "    rrf_scores = {}\n",
    "    for doc_id in all_docs:\n",
    "        vector_score = 1 / (k + vector_ranks.get(doc_id, len(vector_results) + 1))\n",
    "        keyword_score = 1 / (k + keyword_ranks.get(doc_id, len(keyword_results) + 1))\n",
    "        rrf_scores[doc_id] = vector_score + keyword_score\n",
    "    \n",
    "    # Sort by RRF score\n",
    "    sorted_results = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_results\n",
    "\n",
    "def transform_query(query: str) -> str:\n",
    "    \"\"\"Transform query for better retrieval\"\"\"\n",
    "    # Simple query expansion - add related terms\n",
    "    expansions = {\n",
    "        \"cost\": \"pricing billing charges\",\n",
    "        \"performance\": \"speed latency optimization\",\n",
    "        \"security\": \"IAM encryption permissions\",\n",
    "        \"monitoring\": \"CloudWatch metrics logging\"\n",
    "    }\n",
    "    \n",
    "    expanded_query = query\n",
    "    for term, expansion in expansions.items():\n",
    "        if term in query.lower():\n",
    "            expanded_query += f\" {expansion}\"\n",
    "    \n",
    "    return expanded_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query: str, top_k: int = 3) -> List[Dict]:\n",
    "    \"\"\"Perform hybrid search combining vector and keyword search\"\"\"\n",
    "    \n",
    "    # Transform query\n",
    "    expanded_query = transform_query(query)\n",
    "    \n",
    "    print(f\"Original query: {query}\")\n",
    "    if expanded_query != query:\n",
    "        print(f\"Expanded query: {expanded_query}\")\n",
    "    \n",
    "    # Perform both searches\n",
    "    vector_results = vector_search(expanded_query, top_k=5)\n",
    "    keyword_results = keyword_search(expanded_query, top_k=5)\n",
    "    \n",
    "    print(f\"\\nVector search results:\")\n",
    "    for i, (doc_id, score) in enumerate(vector_results[:3], 1):\n",
    "        print(f\"  {i}. {doc_id} (similarity: {score:.3f})\")\n",
    "    \n",
    "    print(f\"\\nKeyword search results:\")\n",
    "    for i, (doc_id, score) in enumerate(keyword_results[:3], 1):\n",
    "        print(f\"  {i}. {doc_id} (BM25: {score:.3f})\")\n",
    "    \n",
    "    # Combine using RRF\n",
    "    fused_results = reciprocal_rank_fusion(vector_results, keyword_results)\n",
    "    \n",
    "    print(f\"\\nHybrid (RRF) results:\")\n",
    "    final_results = []\n",
    "    for i, (doc_id, rrf_score) in enumerate(fused_results[:top_k], 1):\n",
    "        doc = hybrid_index[\"documents\"][doc_id]\n",
    "        print(f\"  {i}. {doc_id} (RRF: {rrf_score:.3f})\")\n",
    "        final_results.append({\n",
    "            'id': doc_id,\n",
    "            'title': doc['title'],\n",
    "            'content': doc['content'],\n",
    "            'rrf_score': rrf_score\n",
    "        })\n",
    "    \n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query: str, context_docs: List[Dict]) -> str:\n",
    "    \"\"\"Generate answer using Nova Pro with hybrid search context\"\"\"\n",
    "    \n",
    "    context_parts = []\n",
    "    for doc in context_docs:\n",
    "        context_parts.append(f\"Title: {doc['title']}\\nContent: {doc['content']}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    prompt = f\"\"\"Based on the following context retrieved through hybrid search (vector + keyword), answer the question accurately.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=GENERATION_MODEL,\n",
    "        body=json.dumps({\n",
    "            \"messages\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"text\": prompt}]\n",
    "            }],\n",
    "            \"inferenceConfig\": {\n",
    "                \"maxTokens\": 300,\n",
    "                \"temperature\": 0.1\n",
    "            }\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response['body'].read())\n",
    "    return result['output']['message']['content'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_hybrid_rag(question: str) -> Dict:\n",
    "    \"\"\"Complete hybrid RAG pipeline\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"HYBRID SEARCH RAG QUERY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Perform hybrid search\n",
    "    retrieved_docs = hybrid_search(question, top_k=3)\n",
    "    \n",
    "    # Generate answer\n",
    "    answer = generate_answer(question, retrieved_docs)\n",
    "    \n",
    "    print(f\"\\nFinal Answer: {answer}\")\n",
    "    print(f\"\\n{'='*60}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': answer,\n",
    "        'sources': [doc['title'] for doc in retrieved_docs],\n",
    "        'rrf_scores': [doc['rrf_score'] for doc in retrieved_docs]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hybrid search RAG\n",
    "test_questions = [\n",
    "    \"What are the Lambda pricing costs?\",\n",
    "    \"How to optimize Lambda performance?\",\n",
    "    \"Lambda security best practices?\",\n",
    "    \"How to monitor Lambda functions?\",\n",
    "    \"Lambda deployment strategies?\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "for question in test_questions:\n",
    "    result = query_hybrid_rag(question)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with vector-only search\n",
    "print(\"COMPARISON: HYBRID vs VECTOR-ONLY SEARCH\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "sample_query = \"Lambda cost optimization\"\n",
    "\n",
    "print(f\"Query: {sample_query}\\n\")\n",
    "\n",
    "# Vector-only results\n",
    "vector_only = vector_search(sample_query, top_k=3)\n",
    "print(\"Vector-only search:\")\n",
    "for i, (doc_id, score) in enumerate(vector_only, 1):\n",
    "    title = hybrid_index[\"documents\"][doc_id][\"title\"]\n",
    "    print(f\"  {i}. {title} (similarity: {score:.3f})\")\n",
    "\n",
    "# Keyword-only results\n",
    "keyword_only = keyword_search(sample_query, top_k=3)\n",
    "print(\"\\nKeyword-only search:\")\n",
    "for i, (doc_id, score) in enumerate(keyword_only, 1):\n",
    "    title = hybrid_index[\"documents\"][doc_id][\"title\"]\n",
    "    print(f\"  {i}. {title} (BM25: {score:.3f})\")\n",
    "\n",
    "# Hybrid results\n",
    "hybrid_only = hybrid_search(sample_query, top_k=3)\n",
    "print(\"\\nHybrid search combines both approaches for better results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Search Benefits\n",
    "\n",
    "### Vector Search (Semantic)\n",
    "✅ **Understands meaning and context**  \n",
    "✅ **Handles synonyms and paraphrasing**  \n",
    "✅ **Good for conceptual queries**  \n",
    "❌ **May miss exact keyword matches**  \n",
    "❌ **Less precise for specific terms**  \n",
    "\n",
    "### Keyword Search (Lexical)\n",
    "✅ **Exact term matching**  \n",
    "✅ **Fast and interpretable**  \n",
    "✅ **Good for specific terminology**  \n",
    "❌ **Misses semantic relationships**  \n",
    "❌ **Struggles with synonyms**  \n",
    "\n",
    "### Hybrid Search (Best of Both)\n",
    "✅ **Combines semantic understanding with exact matching**  \n",
    "✅ **Reciprocal Rank Fusion balances both approaches**  \n",
    "✅ **Query transformation expands search terms**  \n",
    "✅ **Better overall retrieval quality**  \n",
    "\n",
    "### When to Use Hybrid Search:\n",
    "- **Technical documentation** with specific terminology\n",
    "- **Mixed query types** (conceptual + specific)\n",
    "- **High-quality retrieval requirements**\n",
    "- **Domain-specific knowledge bases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nDemo complete! Hybrid search bucket: {HYBRID_BUCKET}\")\n",
    "print(\"Hybrid search provides superior retrieval by combining vector and keyword approaches.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
