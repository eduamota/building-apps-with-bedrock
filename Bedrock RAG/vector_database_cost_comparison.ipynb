{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Database Cost Comparison\n",
    "\n",
    "This notebook compares the costs of three vector storage solutions:\n",
    "- **Amazon S3 Vectors** (Native S3 vector storage)\n",
    "- **Amazon OpenSearch Serverless** (Managed vector search)\n",
    "- **Amazon Aurora PostgreSQL with pgvector** (Relational DB with vector extension)\n",
    "\n",
    "## Installation\n",
    "```bash\n",
    "pip install pandas matplotlib numpy\n",
    "```\n",
    "\n",
    "## Scenario: 10 Million Vectors (1536 dimensions)\n",
    "Typical for a large enterprise knowledge base with OpenAI embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "VECTORS_COUNT = 10_000_000  # 10 million vectors\n",
    "DIMENSIONS = 1536  # OpenAI embedding dimensions\n",
    "QUERIES_PER_MONTH = 1_000_000  # 1 million queries per month\n",
    "UPLOADS_PER_MONTH = 100_000  # 100k new vectors per month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 Vectors Cost Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 Vectors Pricing (US East 1)\n",
    "s3_storage_per_gb_month = 0.023  # $0.023 per GB per month\n",
    "s3_query_per_million = 0.40  # $0.40 per million queries\n",
    "s3_upload_per_million = 0.40  # $0.40 per million uploads\n",
    "\n",
    "# Calculate storage size\n",
    "# Each vector: 1536 dimensions * 4 bytes (float32) = 6,144 bytes\n",
    "# Plus metadata (~500 bytes per vector)\n",
    "bytes_per_vector = (DIMENSIONS * 4) + 500\n",
    "total_gb = (VECTORS_COUNT * bytes_per_vector) / (1024**3)\n",
    "\n",
    "# Monthly costs\n",
    "s3_storage_cost = total_gb * s3_storage_per_gb_month\n",
    "s3_query_cost = (QUERIES_PER_MONTH / 1_000_000) * s3_query_per_million\n",
    "s3_upload_cost = (UPLOADS_PER_MONTH / 1_000_000) * s3_upload_per_million\n",
    "s3_total_monthly = s3_storage_cost + s3_query_cost + s3_upload_cost\n",
    "\n",
    "print(f\"S3 Vectors Cost Breakdown:\")\n",
    "print(f\"Storage: {total_gb:.2f} GB × ${s3_storage_per_gb_month} = ${s3_storage_cost:.2f}/month\")\n",
    "print(f\"Queries: {QUERIES_PER_MONTH:,} × ${s3_query_per_million}/million = ${s3_query_cost:.2f}/month\")\n",
    "print(f\"Uploads: {UPLOADS_PER_MONTH:,} × ${s3_upload_per_million}/million = ${s3_upload_cost:.2f}/month\")\n",
    "print(f\"Total S3 Vectors: ${s3_total_monthly:.2f}/month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenSearch Serverless Cost Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenSearch Serverless Pricing (US East 1)\n",
    "# OCU (OpenSearch Compute Unit) pricing\n",
    "ocu_search_per_hour = 0.24  # $0.24 per OCU-hour for search\n",
    "ocu_indexing_per_hour = 0.24  # $0.24 per OCU-hour for indexing\n",
    "\n",
    "# Estimate OCUs needed for 10M vectors\n",
    "# Rule of thumb: 1 OCU can handle ~1-2M vectors for search\n",
    "search_ocus_needed = max(1, VECTORS_COUNT / 2_000_000)  # Minimum 1 OCU\n",
    "indexing_ocus_needed = 1  # Assume 1 OCU for ongoing indexing\n",
    "\n",
    "hours_per_month = 24 * 30  # 720 hours\n",
    "\n",
    "opensearch_search_cost = search_ocus_needed * ocu_search_per_hour * hours_per_month\n",
    "opensearch_indexing_cost = indexing_ocus_needed * ocu_indexing_per_hour * hours_per_month\n",
    "opensearch_total_monthly = opensearch_search_cost + opensearch_indexing_cost\n",
    "\n",
    "print(f\"OpenSearch Serverless Cost Breakdown:\")\n",
    "print(f\"Search OCUs: {search_ocus_needed:.1f} × ${ocu_search_per_hour} × {hours_per_month} hours = ${opensearch_search_cost:.2f}/month\")\n",
    "print(f\"Indexing OCUs: {indexing_ocus_needed} × ${ocu_indexing_per_hour} × {hours_per_month} hours = ${opensearch_indexing_cost:.2f}/month\")\n",
    "print(f\"Total OpenSearch: ${opensearch_total_monthly:.2f}/month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aurora PostgreSQL with pgvector Cost Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aurora PostgreSQL Pricing (US East 1)\n",
    "# For 10M vectors, we need a substantial instance\n",
    "aurora_instance_type = \"r6g.2xlarge\"  # 8 vCPU, 64 GB RAM\n",
    "aurora_instance_cost_per_hour = 0.58  # $0.58 per hour\n",
    "\n",
    "# Storage costs\n",
    "aurora_storage_per_gb_month = 0.10  # $0.10 per GB per month\n",
    "aurora_io_per_million = 0.20  # $0.20 per million I/O requests\n",
    "\n",
    "# Estimate storage needed (vectors + indexes + metadata)\n",
    "# pgvector stores vectors efficiently, but needs additional space for indexes\n",
    "aurora_storage_gb = total_gb * 1.5  # 50% overhead for indexes and metadata\n",
    "\n",
    "# Monthly costs\n",
    "aurora_compute_cost = aurora_instance_cost_per_hour * hours_per_month\n",
    "aurora_storage_cost = aurora_storage_gb * aurora_storage_per_gb_month\n",
    "# Estimate I/O: queries + background maintenance\n",
    "aurora_io_requests = (QUERIES_PER_MONTH + UPLOADS_PER_MONTH) * 10  # Assume 10 I/O per operation\n",
    "aurora_io_cost = (aurora_io_requests / 1_000_000) * aurora_io_per_million\n",
    "aurora_total_monthly = aurora_compute_cost + aurora_storage_cost + aurora_io_cost\n",
    "\n",
    "print(f\"Aurora PostgreSQL Cost Breakdown:\")\n",
    "print(f\"Compute ({aurora_instance_type}): ${aurora_instance_cost_per_hour} × {hours_per_month} hours = ${aurora_compute_cost:.2f}/month\")\n",
    "print(f\"Storage: {aurora_storage_gb:.2f} GB × ${aurora_storage_per_gb_month} = ${aurora_storage_cost:.2f}/month\")\n",
    "print(f\"I/O: {aurora_io_requests:,} requests × ${aurora_io_per_million}/million = ${aurora_io_cost:.2f}/month\")\n",
    "print(f\"Total Aurora: ${aurora_total_monthly:.2f}/month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = {\n",
    "    'Service': ['S3 Vectors', 'OpenSearch Serverless', 'Aurora PostgreSQL'],\n",
    "    'Monthly Cost ($)': [s3_total_monthly, opensearch_total_monthly, aurora_total_monthly],\n",
    "    'Annual Cost ($)': [s3_total_monthly * 12, opensearch_total_monthly * 12, aurora_total_monthly * 12]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "df['Savings vs S3 (%)'] = ((df['Monthly Cost ($)'] - s3_total_monthly) / s3_total_monthly * 100).round(1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COST COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Calculate savings\n",
    "opensearch_savings = ((opensearch_total_monthly - s3_total_monthly) / opensearch_total_monthly * 100)\n",
    "aurora_savings = ((aurora_total_monthly - s3_total_monthly) / aurora_total_monthly * 100)\n",
    "\n",
    "print(f\"\\nKey Insights:\")\n",
    "print(f\"• S3 Vectors is {opensearch_savings:.0f}% cheaper than OpenSearch Serverless\")\n",
    "print(f\"• S3 Vectors is {aurora_savings:.0f}% cheaper than Aurora PostgreSQL\")\n",
    "print(f\"• Annual savings with S3 Vectors: ${(opensearch_total_monthly - s3_total_monthly) * 12:,.0f} vs OpenSearch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cost comparison chart\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Monthly cost comparison\n",
    "services = df['Service']\n",
    "monthly_costs = df['Monthly Cost ($)']\n",
    "colors = ['#2E8B57', '#FF6B6B', '#4ECDC4']  # Green for S3 (cheapest)\n",
    "\n",
    "bars1 = ax1.bar(services, monthly_costs, color=colors)\n",
    "ax1.set_title('Monthly Cost Comparison\\n(10M Vectors, 1M Queries/month)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Monthly Cost ($)', fontsize=12)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, cost in zip(bars1, monthly_costs):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "             f'${cost:.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Cost breakdown for S3 Vectors\n",
    "s3_components = ['Storage', 'Queries', 'Uploads']\n",
    "s3_costs = [s3_storage_cost, s3_query_cost, s3_upload_cost]\n",
    "colors_s3 = ['#90EE90', '#98FB98', '#F0FFF0']\n",
    "\n",
    "bars2 = ax2.bar(s3_components, s3_costs, color=colors_s3)\n",
    "ax2.set_title('S3 Vectors Cost Breakdown', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Monthly Cost ($)', fontsize=12)\n",
    "\n",
    "# Add value labels\n",
    "for bar, cost in zip(bars2, s3_costs):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "             f'${cost:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print cost per vector\n",
    "print(f\"\\nCost per million vectors stored:\")\n",
    "for service, cost in zip(services, monthly_costs):\n",
    "    cost_per_million = (cost / VECTORS_COUNT) * 1_000_000\n",
    "    print(f\"• {service}: ${cost_per_million:.2f}/month per million vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance & Feature Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature comparison table\n",
    "features_data = {\n",
    "    'Feature': [\n",
    "        'Query Latency',\n",
    "        'Scalability',\n",
    "        'Maintenance',\n",
    "        'Metadata Filtering',\n",
    "        'ACID Transactions',\n",
    "        'Backup/Recovery',\n",
    "        'Multi-AZ',\n",
    "        'Setup Complexity'\n",
    "    ],\n",
    "    'S3 Vectors': [\n",
    "        'Sub-second (cold), ~100ms (warm)',\n",
    "        'Unlimited (2B vectors/index)',\n",
    "        'Fully managed',\n",
    "        'Rich metadata support',\n",
    "        'No',\n",
    "        'Built-in S3 durability',\n",
    "        'Yes (S3 cross-region)',\n",
    "        'Low (native S3)'\n",
    "    ],\n",
    "    'OpenSearch Serverless': [\n",
    "        '~50-200ms',\n",
    "        'Auto-scaling OCUs',\n",
    "        'Fully managed',\n",
    "        'Advanced filtering',\n",
    "        'No',\n",
    "        'Automated snapshots',\n",
    "        'Yes',\n",
    "        'Medium (collection setup)'\n",
    "    ],\n",
    "    'Aurora PostgreSQL': [\n",
    "        '~10-50ms',\n",
    "        'Limited by instance size',\n",
    "        'Managed DB, manual tuning',\n",
    "        'SQL-based filtering',\n",
    "        'Yes (full ACID)',\n",
    "        'Point-in-time recovery',\n",
    "        'Yes',\n",
    "        'High (DB + pgvector setup)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "features_df = pd.DataFrame(features_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(features_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations\n",
    "\n",
    "### Choose S3 Vectors when:\n",
    "- **Cost is a primary concern** (90%+ savings)\n",
    "- You need **massive scale** (billions of vectors)\n",
    "- **Subsecond latency is acceptable**\n",
    "- You want **zero infrastructure management**\n",
    "- Building **new RAG applications**\n",
    "\n",
    "### Choose OpenSearch Serverless when:\n",
    "- You need **advanced search features** (faceting, aggregations)\n",
    "- **Hybrid search** (vector + text) is required\n",
    "- You have **existing OpenSearch expertise**\n",
    "- **Complex analytics** on vector data\n",
    "\n",
    "### Choose Aurora PostgreSQL when:\n",
    "- You need **ACID transactions**\n",
    "- **Existing PostgreSQL applications** to extend\n",
    "- **Complex relational queries** with vectors\n",
    "- **Lowest possible latency** is critical\n",
    "- **Hybrid workloads** (transactional + vector)\n",
    "\n",
    "## Cost Scaling Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cost scaling with different vector counts\n",
    "vector_counts = [1_000_000, 5_000_000, 10_000_000, 50_000_000, 100_000_000]\n",
    "scaling_results = []\n",
    "\n",
    "for count in vector_counts:\n",
    "    # S3 Vectors scaling\n",
    "    gb = (count * bytes_per_vector) / (1024**3)\n",
    "    s3_cost = gb * s3_storage_per_gb_month + s3_query_cost + s3_upload_cost\n",
    "    \n",
    "    # OpenSearch scaling (more OCUs needed)\n",
    "    search_ocus = max(1, count / 2_000_000)\n",
    "    os_cost = (search_ocus + 1) * ocu_search_per_hour * hours_per_month\n",
    "    \n",
    "    # Aurora scaling (larger instances needed)\n",
    "    if count <= 10_000_000:\n",
    "        aurora_cost = aurora_total_monthly\n",
    "    elif count <= 50_000_000:\n",
    "        aurora_cost = aurora_total_monthly * 2  # Larger instance\n",
    "    else:\n",
    "        aurora_cost = aurora_total_monthly * 4  # Much larger instance\n",
    "    \n",
    "    scaling_results.append({\n",
    "        'Vectors (M)': count / 1_000_000,\n",
    "        'S3 Vectors': s3_cost,\n",
    "        'OpenSearch': os_cost,\n",
    "        'Aurora': aurora_cost\n",
    "    })\n",
    "\n",
    "scaling_df = pd.DataFrame(scaling_results)\n",
    "\n",
    "# Plot scaling comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(scaling_df['Vectors (M)'], scaling_df['S3 Vectors'], 'o-', label='S3 Vectors', linewidth=3, markersize=8)\n",
    "plt.plot(scaling_df['Vectors (M)'], scaling_df['OpenSearch'], 's-', label='OpenSearch Serverless', linewidth=3, markersize=8)\n",
    "plt.plot(scaling_df['Vectors (M)'], scaling_df['Aurora'], '^-', label='Aurora PostgreSQL', linewidth=3, markersize=8)\n",
    "\n",
    "plt.xlabel('Vector Count (Millions)', fontsize=12)\n",
    "plt.ylabel('Monthly Cost ($)', fontsize=12)\n",
    "plt.title('Cost Scaling Comparison\\n(1M queries/month, 100K uploads/month)', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nScaling Analysis:\")\n",
    "print(scaling_df.to_string(index=False))\n",
    "\n",
    "# Calculate savings at 100M vectors\n",
    "s3_100m = scaling_df.iloc[-1]['S3 Vectors']\n",
    "os_100m = scaling_df.iloc[-1]['OpenSearch']\n",
    "aurora_100m = scaling_df.iloc[-1]['Aurora']\n",
    "\n",
    "print(f\"\\nAt 100M vectors:\")\n",
    "print(f\"• S3 Vectors saves ${(os_100m - s3_100m) * 12:,.0f}/year vs OpenSearch\")\n",
    "print(f\"• S3 Vectors saves ${(aurora_100m - s3_100m) * 12:,.0f}/year vs Aurora\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
