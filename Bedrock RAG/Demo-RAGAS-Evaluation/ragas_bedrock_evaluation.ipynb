{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGAS Evaluation for Bedrock RAG Solutions\n",
    "Automated evaluation of RAG pipeline performance using RAGAS metrics\n",
    "\n",
    "**RAGAS Metrics:**\n",
    "- **Faithfulness**: Answer consistency with retrieved context\n",
    "- **Answer Relevancy**: How relevant the answer is to the question\n",
    "- **Context Precision**: Relevance of retrieved chunks to query\n",
    "- **Context Recall**: Coverage of necessary information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install RAGAS if not already installed\n",
    "!pip install ragas datasets pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall\n",
    ")\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bedrock clients\n",
    "bedrock_runtime = boto3.client('bedrock-runtime')\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Configuration\n",
    "RAGAS_BUCKET = f\"ragas-evaluation-{int(time.time())}\"\n",
    "EMBEDDING_MODEL = \"amazon.titan-embed-text-v1\"\n",
    "GENERATION_MODEL = \"amazon.nova-pro-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create S3 bucket for evaluation data\n",
    "s3.create_bucket(Bucket=RAGAS_BUCKET)\n",
    "print(f\"Created RAGAS evaluation bucket: {RAGAS_BUCKET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample knowledge base for evaluation\n",
    "knowledge_base = [\n",
    "    {\n",
    "        \"id\": \"lambda_pricing\",\n",
    "        \"content\": \"AWS Lambda pricing is based on requests and compute time. You pay $0.20 per 1M requests and $0.0000166667 per GB-second. The first 1M requests per month are free.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lambda_memory\",\n",
    "        \"content\": \"Lambda memory can be configured from 128 MB to 10,240 MB in 1 MB increments. CPU power scales linearly with memory allocation.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lambda_timeout\",\n",
    "        \"content\": \"Lambda functions have a maximum execution time of 15 minutes (900 seconds). The default timeout is 3 seconds.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lambda_coldstart\",\n",
    "        \"content\": \"Cold starts occur when Lambda initializes a new execution environment. This adds latency. Use provisioned concurrency to eliminate cold starts.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lambda_vpc\",\n",
    "        \"content\": \"Lambda functions can access VPC resources like RDS databases. VPC configuration adds cold start latency. Use VPC endpoints for AWS services.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Knowledge base loaded with {len(knowledge_base)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Get embedding using Titan model\"\"\"\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=EMBEDDING_MODEL,\n",
    "        body=json.dumps({\"inputText\": text})\n",
    "    )\n",
    "    return json.loads(response['body'].read())['embedding']\n",
    "\n",
    "def cosine_similarity(a: List[float], b: List[float]) -> float:\n",
    "    \"\"\"Calculate cosine similarity\"\"\"\n",
    "    a_np = np.array(a)\n",
    "    b_np = np.array(b)\n",
    "    return np.dot(a_np, b_np) / (np.linalg.norm(a_np) * np.linalg.norm(b_np))\n",
    "\n",
    "# Create embeddings for knowledge base\n",
    "print(\"Creating embeddings...\")\n",
    "for doc in knowledge_base:\n",
    "    doc[\"embedding\"] = get_embedding(doc[\"content\"])\n",
    "    time.sleep(0.1)\n",
    "print(\"Embeddings created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(query: str, top_k: int = 3) -> List[str]:\n",
    "    \"\"\"Retrieve relevant context for query\"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    similarities = []\n",
    "    for doc in knowledge_base:\n",
    "        similarity = cosine_similarity(query_embedding, doc[\"embedding\"])\n",
    "        similarities.append((doc[\"content\"], similarity))\n",
    "    \n",
    "    # Sort by similarity and return top_k contexts\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [content for content, _ in similarities[:top_k]]\n",
    "\n",
    "def generate_answer(query: str, contexts: List[str]) -> str:\n",
    "    \"\"\"Generate answer using Nova Pro\"\"\"\n",
    "    context_text = \"\\n\\n\".join(contexts)\n",
    "    \n",
    "    prompt = f\"\"\"Based on the following context, answer the question accurately.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=GENERATION_MODEL,\n",
    "        body=json.dumps({\n",
    "            \"messages\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"text\": prompt}]\n",
    "            }],\n",
    "            \"inferenceConfig\": {\n",
    "                \"maxTokens\": 300,\n",
    "                \"temperature\": 0.1\n",
    "            }\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response['body'].read())\n",
    "    return result['output']['message']['content'][0]['text']\n",
    "\n",
    "def bedrock_rag_pipeline(query: str) -> Dict:\n",
    "    \"\"\"Complete Bedrock RAG pipeline\"\"\"\n",
    "    contexts = retrieve_context(query, top_k=3)\n",
    "    answer = generate_answer(query, contexts)\n",
    "    \n",
    "    return {\n",
    "        \"question\": query,\n",
    "        \"contexts\": contexts,\n",
    "        \"answer\": answer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation dataset\n",
    "test_questions = [\n",
    "    \"How much does AWS Lambda cost?\",\n",
    "    \"What is the maximum memory for Lambda functions?\",\n",
    "    \"What is the maximum execution time for Lambda?\",\n",
    "    \"How can I reduce Lambda cold starts?\",\n",
    "    \"Can Lambda functions access VPC resources?\",\n",
    "    \"What is the default timeout for Lambda functions?\",\n",
    "    \"How does Lambda pricing work?\",\n",
    "    \"What causes Lambda cold starts?\"\n",
    "]\n",
    "\n",
    "# Ground truth answers for reference (optional for RAGAS)\n",
    "ground_truth = [\n",
    "    \"Lambda costs $0.20 per 1M requests and $0.0000166667 per GB-second, with 1M free requests monthly.\",\n",
    "    \"Lambda memory can be configured up to 10,240 MB in 1 MB increments.\",\n",
    "    \"Lambda functions have a maximum execution time of 15 minutes (900 seconds).\",\n",
    "    \"Use provisioned concurrency to eliminate cold starts for Lambda functions.\",\n",
    "    \"Yes, Lambda functions can access VPC resources like RDS databases, but this adds cold start latency.\",\n",
    "    \"The default timeout for Lambda functions is 3 seconds.\",\n",
    "    \"Lambda pricing is based on the number of requests and compute time (GB-seconds).\",\n",
    "    \"Cold starts occur when Lambda initializes a new execution environment after being idle.\"\n",
    "]\n",
    "\n",
    "print(f\"Created evaluation dataset with {len(test_questions)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate RAG responses for evaluation\n",
    "print(\"Generating RAG responses...\")\n",
    "evaluation_data = []\n",
    "\n",
    "for i, question in enumerate(test_questions):\n",
    "    print(f\"Processing question {i+1}/{len(test_questions)}: {question[:50]}...\")\n",
    "    \n",
    "    # Get RAG response\n",
    "    rag_result = bedrock_rag_pipeline(question)\n",
    "    \n",
    "    evaluation_data.append({\n",
    "        \"question\": question,\n",
    "        \"contexts\": rag_result[\"contexts\"],\n",
    "        \"answer\": rag_result[\"answer\"],\n",
    "        \"ground_truth\": ground_truth[i] if i < len(ground_truth) else \"\"\n",
    "    })\n",
    "    \n",
    "    time.sleep(0.5)  # Rate limiting\n",
    "\n",
    "print(\"RAG responses generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to RAGAS dataset format\n",
    "ragas_dataset = Dataset.from_dict({\n",
    "    \"question\": [item[\"question\"] for item in evaluation_data],\n",
    "    \"contexts\": [item[\"contexts\"] for item in evaluation_data],\n",
    "    \"answer\": [item[\"answer\"] for item in evaluation_data],\n",
    "    \"ground_truth\": [item[\"ground_truth\"] for item in evaluation_data]\n",
    "})\n",
    "\n",
    "print(f\"Created RAGAS dataset with {len(ragas_dataset)} samples\")\n",
    "print(\"\\nSample data:\")\n",
    "print(f\"Question: {ragas_dataset[0]['question']}\")\n",
    "print(f\"Answer: {ragas_dataset[0]['answer'][:100]}...\")\n",
    "print(f\"Contexts: {len(ragas_dataset[0]['contexts'])} retrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure RAGAS to use Bedrock models\n",
    "import os\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_aws import BedrockLLM, BedrockEmbeddings\n",
    "\n",
    "# Note: This is a simplified approach. In practice, you might need to configure\n",
    "# RAGAS to work with Bedrock models directly or use OpenAI for evaluation\n",
    "\n",
    "# For this demo, we'll use the default RAGAS configuration\n",
    "# In production, configure with your preferred evaluation models\n",
    "\n",
    "print(\"RAGAS configuration ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RAGAS evaluation\n",
    "print(\"Running RAGAS evaluation...\")\n",
    "print(\"Note: This requires OpenAI API key or custom model configuration\")\n",
    "\n",
    "# Metrics to evaluate\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Run evaluation\n",
    "    result = evaluate(\n",
    "        dataset=ragas_dataset,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    \n",
    "    print(\"\\nRAGAS Evaluation Results:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for metric_name, score in result.items():\n",
    "        print(f\"{metric_name}: {score:.4f}\")\n",
    "    \n",
    "    # Convert to DataFrame for detailed analysis\n",
    "    results_df = result.to_pandas()\n",
    "    print(f\"\\nDetailed results saved to DataFrame with {len(results_df)} rows\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"RAGAS evaluation error: {e}\")\n",
    "    print(\"This typically requires OpenAI API configuration or custom model setup\")\n",
    "    \n",
    "    # Fallback: Manual evaluation demonstration\n",
    "    print(\"\\nDemonstrating manual evaluation approach...\")\n",
    "    \n",
    "    # Simple manual evaluation metrics\n",
    "    manual_scores = {\n",
    "        \"faithfulness\": [],\n",
    "        \"answer_relevancy\": [],\n",
    "        \"context_precision\": [],\n",
    "        \"context_recall\": []\n",
    "    }\n",
    "    \n",
    "    for item in evaluation_data:\n",
    "        # Simple heuristic scoring (0-1 scale)\n",
    "        answer_length = len(item[\"answer\"].split())\n",
    "        context_count = len(item[\"contexts\"])\n",
    "        \n",
    "        # Mock scores based on simple heuristics\n",
    "        manual_scores[\"faithfulness\"].append(min(1.0, answer_length / 50))\n",
    "        manual_scores[\"answer_relevancy\"].append(min(1.0, answer_length / 30))\n",
    "        manual_scores[\"context_precision\"].append(min(1.0, context_count / 3))\n",
    "        manual_scores[\"context_recall\"].append(min(1.0, context_count / 3))\n",
    "    \n",
    "    print(\"\\nManual Evaluation Results (Heuristic):\")\n",
    "    print(\"=\" * 40)\n",
    "    for metric, scores in manual_scores.items():\n",
    "        avg_score = sum(scores) / len(scores)\n",
    "        print(f\"{metric}: {avg_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results by question\n",
    "print(\"\\nPER-QUESTION ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, item in enumerate(evaluation_data):\n",
    "    print(f\"\\nQuestion {i+1}: {item['question']}\")\n",
    "    print(f\"Answer: {item['answer'][:100]}...\")\n",
    "    print(f\"Contexts retrieved: {len(item['contexts'])}\")\n",
    "    print(f\"Answer length: {len(item['answer'].split())} words\")\n",
    "    \n",
    "    # Show first context for reference\n",
    "    if item['contexts']:\n",
    "        print(f\"Top context: {item['contexts'][0][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different RAG configurations (simulation)\n",
    "print(\"\\nRAG CONFIGURATION COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Simulate different configurations\n",
    "configurations = {\n",
    "    \"Basic RAG (top-3)\": {\"faithfulness\": 0.85, \"answer_relevancy\": 0.82, \"context_precision\": 0.78, \"context_recall\": 0.80},\n",
    "    \"Hybrid Search\": {\"faithfulness\": 0.88, \"answer_relevancy\": 0.85, \"context_precision\": 0.82, \"context_recall\": 0.84},\n",
    "    \"Re-ranking Pipeline\": {\"faithfulness\": 0.91, \"answer_relevancy\": 0.89, \"context_precision\": 0.87, \"context_recall\": 0.85},\n",
    "    \"Multi-Collection\": {\"faithfulness\": 0.87, \"answer_relevancy\": 0.86, \"context_precision\": 0.84, \"context_recall\": 0.83}\n",
    "}\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(configurations).T\n",
    "print(comparison_df)\n",
    "\n",
    "# Calculate overall scores\n",
    "comparison_df['Overall'] = comparison_df.mean(axis=1)\n",
    "print(\"\\nOverall Scores:\")\n",
    "print(comparison_df['Overall'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGAS Metrics Explained\n",
    "\n",
    "### **Faithfulness** (0-1 scale)\n",
    "- Measures if the generated answer is factually consistent with retrieved context\n",
    "- Higher scores indicate answers grounded in provided evidence\n",
    "- **Good**: Answer claims can be verified from context\n",
    "- **Bad**: Answer contains information not in context\n",
    "\n",
    "### **Answer Relevancy** (0-1 scale)\n",
    "- Evaluates how relevant the answer is to the original question\n",
    "- Higher scores indicate direct, on-topic responses\n",
    "- **Good**: Answer directly addresses the question\n",
    "- **Bad**: Answer is off-topic or too generic\n",
    "\n",
    "### **Context Precision** (0-1 scale)\n",
    "- Measures the proportion of relevant chunks in retrieved contexts\n",
    "- Higher scores indicate better retrieval quality\n",
    "- **Good**: Most retrieved contexts are relevant to query\n",
    "- **Bad**: Many irrelevant contexts retrieved\n",
    "\n",
    "### **Context Recall** (0-1 scale)\n",
    "- Determines if all necessary information was retrieved\n",
    "- Higher scores indicate comprehensive information retrieval\n",
    "- **Good**: All information needed to answer is retrieved\n",
    "- **Bad**: Missing key information in retrieved contexts\n",
    "\n",
    "## Interpretation Guidelines:\n",
    "- **0.8+**: Excellent performance\n",
    "- **0.6-0.8**: Good performance\n",
    "- **0.4-0.6**: Needs improvement\n",
    "- **<0.4**: Poor performance, requires optimization\n",
    "\n",
    "## Using RAGAS for RAG Optimization:\n",
    "1. **Low Faithfulness**: Improve context relevance or generation prompts\n",
    "2. **Low Answer Relevancy**: Refine query understanding or response generation\n",
    "3. **Low Context Precision**: Improve retrieval algorithms or embeddings\n",
    "4. **Low Context Recall**: Increase retrieval count or improve chunking strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results\n",
    "evaluation_summary = {\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"total_questions\": len(test_questions),\n",
    "    \"knowledge_base_size\": len(knowledge_base),\n",
    "    \"embedding_model\": EMBEDDING_MODEL,\n",
    "    \"generation_model\": GENERATION_MODEL,\n",
    "    \"evaluation_data\": evaluation_data\n",
    "}\n",
    "\n",
    "# Store in S3\n",
    "s3.put_object(\n",
    "    Bucket=RAGAS_BUCKET,\n",
    "    Key=\"evaluation_results.json\",\n",
    "    Body=json.dumps(evaluation_summary, indent=2)\n",
    ")\n",
    "\n",
    "print(f\"\\nEvaluation complete!\")\n",
    "print(f\"Results saved to S3 bucket: {RAGAS_BUCKET}\")\n",
    "print(f\"\\nRAGAS provides objective metrics to:\")\n",
    "print(f\"- Compare different RAG configurations\")\n",
    "print(f\"- Monitor production RAG performance\")\n",
    "print(f\"- Identify areas for improvement\")\n",
    "print(f\"- A/B test RAG optimizations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
