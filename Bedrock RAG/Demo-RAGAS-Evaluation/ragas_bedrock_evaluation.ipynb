{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGAS Evaluation for Bedrock RAG Solutions\n",
    "Automated evaluation of RAG pipeline performance using RAGAS metrics\n",
    "\n",
    "**RAGAS Metrics:**\n",
    "- **Faithfulness**: Answer consistency with retrieved context\n",
    "- **Answer Relevancy**: How relevant the answer is to the question\n",
    "- **Context Precision**: Relevance of retrieved chunks to query\n",
    "- **Context Recall**: Coverage of necessary information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ragas\n",
      "  Downloading ragas-0.4.2-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.4.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: pandas in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.21.0 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from ragas) (2.3.5)\n",
      "Collecting tiktoken (from ragas)\n",
      "  Downloading tiktoken-0.12.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from ragas) (2.12.5)\n",
      "Requirement already satisfied: nest-asyncio in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from ragas) (1.6.0)\n",
      "Collecting appdirs (from ragas)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting diskcache>=5.6.3 (from ragas)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: typer in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from ragas) (0.20.0)\n",
      "Requirement already satisfied: rich in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from ragas) (14.2.0)\n",
      "Collecting openai>=1.0.0 (from ragas)\n",
      "  Downloading openai-2.14.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting tqdm (from ragas)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting instructor (from ragas)\n",
      "  Downloading instructor-1.13.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pillow>=10.4.0 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from ragas) (11.3.0)\n",
      "Collecting networkx (from ragas)\n",
      "  Downloading networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting scikit-network (from ragas)\n",
      "  Downloading scikit_network-0.33.5-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.5 kB)\n",
      "Collecting langchain (from ragas)\n",
      "  Downloading langchain-1.2.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting langchain-core (from ragas)\n",
      "  Downloading langchain_core-1.2.6-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting langchain-community (from ragas)\n",
      "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain_openai (from ragas)\n",
      "  Downloading langchain_openai-1.1.6-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: filelock in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from datasets) (0.28.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py313-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.10.0,>=2023.1.0 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting huggingface-hub<2.0,>=0.25.0 (from datasets)\n",
      "  Downloading huggingface_hub-1.2.4-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: shellingham in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Collecting typer-slim (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading typer_slim-0.21.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.0.0->ragas)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.10.0 (from openai>=1.0.0->ragas)\n",
      "  Downloading jiter-0.12.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from openai>=1.0.0->ragas)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from pydantic>=2.0.0->ragas) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from pydantic>=2.0.0->ragas) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from pydantic>=2.0.0->ragas) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.6.0)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.16 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from instructor->ragas) (0.17.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.4 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from instructor->ragas) (3.1.6)\n",
      "Collecting jiter<1,>=0.10.0 (from openai>=1.0.0->ragas)\n",
      "  Downloading jiter-0.11.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pre-commit>=4.3.0 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from instructor->ragas) (4.5.0)\n",
      "Requirement already satisfied: tenacity<10.0.0,>=8.2.3 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from instructor->ragas) (9.1.2)\n",
      "Collecting ty>=0.0.1a23 (from instructor->ragas)\n",
      "  Downloading ty-0.0.9-py3-none-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from jinja2<4.0.0,>=3.1.4->instructor->ragas) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from rich->ragas) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from rich->ragas) (2.19.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from typer->ragas) (8.3.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich->ragas) (0.1.2)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from pre-commit>=4.3.0->instructor->ragas) (3.5.0)\n",
      "Requirement already satisfied: identify>=1.0.0 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from pre-commit>=4.3.0->instructor->ragas) (2.6.15)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from pre-commit>=4.3.0->instructor->ragas) (1.9.1)\n",
      "Requirement already satisfied: virtualenv>=20.10.0 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from pre-commit>=4.3.0->instructor->ragas) (20.35.4)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from virtualenv>=20.10.0->pre-commit>=4.3.0->instructor->ragas) (0.4.0)\n",
      "Requirement already satisfied: platformdirs<5,>=3.9.1 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from virtualenv>=20.10.0->pre-commit>=4.3.0->instructor->ragas) (4.5.1)\n",
      "Collecting langgraph<1.1.0,>=1.0.2 (from langchain->ragas)\n",
      "  Downloading langgraph-1.0.5-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core->ragas)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core->ragas)\n",
      "  Downloading langsmith-0.6.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting uuid-utils<1.0,>=0.12.0 (from langchain-core->ragas)\n",
      "  Downloading uuid_utils-0.12.0-cp39-abi3-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (1.1 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0.0,>=1.33.0->langchain-core->ragas)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph<1.1.0,>=1.0.2->langchain->ragas)\n",
      "  Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph<1.1.0,>=1.0.2->langchain->ragas)\n",
      "  Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting langgraph-sdk<0.4.0,>=0.3.0 (from langgraph<1.1.0,>=1.0.2->langchain->ragas)\n",
      "  Downloading langgraph_sdk-0.3.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain->ragas)\n",
      "  Downloading ormsgpack-1.12.1-cp313-cp313-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (3.2 kB)\n",
      "Collecting orjson>=3.10.1 (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain->ragas)\n",
      "  Downloading orjson-3.11.5-cp313-cp313-macosx_15_0_arm64.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core->ragas)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.3.45->langchain-core->ragas)\n",
      "  Downloading zstandard-0.25.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community->ragas)\n",
      "  Downloading langchain_classic-1.0.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting SQLAlchemy<3.0.0,>=1.4.0 (from langchain-community->ragas)\n",
      "  Downloading sqlalchemy-2.0.45-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community->ragas)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from langchain-community->ragas) (2.12.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from langchain-community->ragas) (0.4.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community->ragas)\n",
      "  Downloading marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community->ragas)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting langchain-text-splitters<2.0.0,>=1.1.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community->ragas)\n",
      "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community->ragas) (1.2.1)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community->ragas)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken->ragas)\n",
      "  Downloading regex-2025.11.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: scipy>=1.7.3 in /Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages (from scikit-network->ragas) (1.16.3)\n",
      "Downloading ragas-0.4.2-py3-none-any.whl (457 kB)\n",
      "Downloading datasets-4.4.2-py3-none-any.whl (512 kB)\n",
      "Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Downloading huggingface_hub-1.2.4-py3-none-any.whl (520 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.18-py313-none-any.whl (151 kB)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Downloading openai-2.14.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading instructor-1.13.0-py3-none-any.whl (160 kB)\n",
      "Downloading jiter-0.11.1-cp313-cp313-macosx_11_0_arm64.whl (314 kB)\n",
      "Downloading ty-0.0.9-py3-none-macosx_11_0_arm64.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain-1.2.1-py3-none-any.whl (105 kB)\n",
      "Downloading langchain_core-1.2.6-py3-none-any.whl (489 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langgraph-1.0.5-py3-none-any.whl (157 kB)\n",
      "Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
      "Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl (35 kB)\n",
      "Downloading langgraph_sdk-0.3.1-py3-none-any.whl (66 kB)\n",
      "Downloading langsmith-0.6.1-py3-none-any.whl (282 kB)\n",
      "Downloading uuid_utils-0.12.0-cp39-abi3-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (603 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m603.2/603.2 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading orjson-3.11.5-cp313-cp313-macosx_15_0_arm64.whl (129 kB)\n",
      "Downloading ormsgpack-1.12.1-cp313-cp313-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (376 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading xxhash-3.6.0-cp313-cp313-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading zstandard-0.25.0-cp313-cp313-macosx_11_0_arm64.whl (640 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m640.4/640.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading langchain_classic-1.0.1-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
      "Downloading marshmallow-3.26.2-py3-none-any.whl (50 kB)\n",
      "Downloading sqlalchemy-2.0.45-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading langchain_openai-1.1.6-py3-none-any.whl (84 kB)\n",
      "Downloading tiktoken-0.12.0-cp313-cp313-macosx_11_0_arm64.whl (993 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m994.0/994.0 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.11.3-cp313-cp313-macosx_11_0_arm64.whl (288 kB)\n",
      "Downloading networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_network-0.33.5-cp313-cp313-macosx_11_0_arm64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading typer_slim-0.21.1-py3-none-any.whl (47 kB)\n",
      "Installing collected packages: appdirs, zstandard, xxhash, uuid-utils, typer-slim, ty, tqdm, SQLAlchemy, sniffio, regex, ormsgpack, orjson, networkx, mypy-extensions, multiprocess, marshmallow, jsonpointer, jiter, hf-xet, fsspec, distro, diskcache, typing-inspect, tiktoken, scikit-network, requests-toolbelt, jsonpatch, openai, langsmith, langgraph-sdk, huggingface-hub, dataclasses-json, langchain-core, instructor, datasets, langgraph-checkpoint, langchain-text-splitters, langchain_openai, langgraph-prebuilt, langchain-classic, langgraph, langchain-community, langchain, ragas\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44/44\u001b[0m [ragas]m [ragas]m [langchain-community]\n",
      "\u001b[1A\u001b[2KSuccessfully installed SQLAlchemy-2.0.45 appdirs-1.4.4 dataclasses-json-0.6.7 datasets-4.4.2 diskcache-5.6.3 distro-1.9.0 fsspec-2025.10.0 hf-xet-1.2.0 huggingface-hub-1.2.4 instructor-1.13.0 jiter-0.11.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-1.2.1 langchain-classic-1.0.1 langchain-community-0.4.1 langchain-core-1.2.6 langchain-text-splitters-1.1.0 langchain_openai-1.1.6 langgraph-1.0.5 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.5 langgraph-sdk-0.3.1 langsmith-0.6.1 marshmallow-3.26.2 multiprocess-0.70.18 mypy-extensions-1.1.0 networkx-3.6.1 openai-2.14.0 orjson-3.11.5 ormsgpack-1.12.1 ragas-0.4.2 regex-2025.11.3 requests-toolbelt-1.0.0 scikit-network-0.33.5 sniffio-1.3.1 tiktoken-0.12.0 tqdm-4.67.1 ty-0.0.9 typer-slim-0.21.1 typing-inspect-0.9.0 uuid-utils-0.12.0 xxhash-3.6.0 zstandard-0.25.0\n"
     ]
    }
   ],
   "source": [
    "# Install RAGAS if not already installed\n",
    "!pip install ragas datasets pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emota/miniconda3/envs/oreilly/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/jn/py_6v4j90519z5d9j8hh442r0000gn/T/ipykernel_86496/2668963657.py:8: DeprecationWarning: Importing faithfulness from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import faithfulness\n",
      "  from ragas.metrics import (\n",
      "/var/folders/jn/py_6v4j90519z5d9j8hh442r0000gn/T/ipykernel_86496/2668963657.py:8: DeprecationWarning: Importing answer_relevancy from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import answer_relevancy\n",
      "  from ragas.metrics import (\n",
      "/var/folders/jn/py_6v4j90519z5d9j8hh442r0000gn/T/ipykernel_86496/2668963657.py:8: DeprecationWarning: Importing context_precision from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_precision\n",
      "  from ragas.metrics import (\n",
      "/var/folders/jn/py_6v4j90519z5d9j8hh442r0000gn/T/ipykernel_86496/2668963657.py:8: DeprecationWarning: Importing context_recall from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_recall\n",
      "  from ragas.metrics import (\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall\n",
    ")\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bedrock clients\n",
    "bedrock_runtime = boto3.client('bedrock-runtime')\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Configuration\n",
    "RAGAS_BUCKET = f\"ragas-evaluation-{int(time.time())}\"\n",
    "EMBEDDING_MODEL = \"amazon.titan-embed-text-v1\"\n",
    "GENERATION_MODEL = \"amazon.nova-pro-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created RAGAS evaluation bucket: ragas-evaluation-1767770912\n"
     ]
    }
   ],
   "source": [
    "# Create S3 bucket for evaluation data\n",
    "s3.create_bucket(Bucket=RAGAS_BUCKET)\n",
    "print(f\"Created RAGAS evaluation bucket: {RAGAS_BUCKET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base loaded with 5 documents\n"
     ]
    }
   ],
   "source": [
    "# Sample knowledge base for evaluation\n",
    "knowledge_base = [\n",
    "    {\n",
    "        \"id\": \"lambda_pricing\",\n",
    "        \"content\": \"AWS Lambda pricing is based on requests and compute time. You pay $0.20 per 1M requests and $0.0000166667 per GB-second. The first 1M requests per month are free.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lambda_memory\",\n",
    "        \"content\": \"Lambda memory can be configured from 128 MB to 10,240 MB in 1 MB increments. CPU power scales linearly with memory allocation.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lambda_timeout\",\n",
    "        \"content\": \"Lambda functions have a maximum execution time of 15 minutes (900 seconds). The default timeout is 3 seconds.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lambda_coldstart\",\n",
    "        \"content\": \"Cold starts occur when Lambda initializes a new execution environment. This adds latency. Use provisioned concurrency to eliminate cold starts.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lambda_vpc\",\n",
    "        \"content\": \"Lambda functions can access VPC resources like RDS databases. VPC configuration adds cold start latency. Use VPC endpoints for AWS services.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Knowledge base loaded with {len(knowledge_base)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings...\n",
      "Embeddings created\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Get embedding using Titan model\"\"\"\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=EMBEDDING_MODEL,\n",
    "        body=json.dumps({\"inputText\": text})\n",
    "    )\n",
    "    return json.loads(response['body'].read())['embedding']\n",
    "\n",
    "def cosine_similarity(a: List[float], b: List[float]) -> float:\n",
    "    \"\"\"Calculate cosine similarity\"\"\"\n",
    "    a_np = np.array(a)\n",
    "    b_np = np.array(b)\n",
    "    return np.dot(a_np, b_np) / (np.linalg.norm(a_np) * np.linalg.norm(b_np))\n",
    "\n",
    "# Create embeddings for knowledge base\n",
    "print(\"Creating embeddings...\")\n",
    "for doc in knowledge_base:\n",
    "    doc[\"embedding\"] = get_embedding(doc[\"content\"])\n",
    "    time.sleep(0.1)\n",
    "print(\"Embeddings created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(query: str, top_k: int = 3) -> List[str]:\n",
    "    \"\"\"Retrieve relevant context for query\"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    similarities = []\n",
    "    for doc in knowledge_base:\n",
    "        similarity = cosine_similarity(query_embedding, doc[\"embedding\"])\n",
    "        similarities.append((doc[\"content\"], similarity))\n",
    "    \n",
    "    # Sort by similarity and return top_k contexts\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [content for content, _ in similarities[:top_k]]\n",
    "\n",
    "def generate_answer(query: str, contexts: List[str]) -> str:\n",
    "    \"\"\"Generate answer using Nova Pro\"\"\"\n",
    "    context_text = \"\\n\\n\".join(contexts)\n",
    "    \n",
    "    prompt = f\"\"\"Based on the following context, answer the question accurately.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=GENERATION_MODEL,\n",
    "        body=json.dumps({\n",
    "            \"messages\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"text\": prompt}]\n",
    "            }],\n",
    "            \"inferenceConfig\": {\n",
    "                \"maxTokens\": 300,\n",
    "                \"temperature\": 0.1\n",
    "            }\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response['body'].read())\n",
    "    return result['output']['message']['content'][0]['text']\n",
    "\n",
    "def bedrock_rag_pipeline(query: str) -> Dict:\n",
    "    \"\"\"Complete Bedrock RAG pipeline\"\"\"\n",
    "    contexts = retrieve_context(query, top_k=3)\n",
    "    answer = generate_answer(query, contexts)\n",
    "    \n",
    "    return {\n",
    "        \"question\": query,\n",
    "        \"contexts\": contexts,\n",
    "        \"answer\": answer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created evaluation dataset with 8 questions\n"
     ]
    }
   ],
   "source": [
    "# Create evaluation dataset\n",
    "test_questions = [\n",
    "    \"How much does AWS Lambda cost?\",\n",
    "    \"What is the maximum memory for Lambda functions?\",\n",
    "    \"What is the maximum execution time for Lambda?\",\n",
    "    \"How can I reduce Lambda cold starts?\",\n",
    "    \"Can Lambda functions access VPC resources?\",\n",
    "    \"What is the default timeout for Lambda functions?\",\n",
    "    \"How does Lambda pricing work?\",\n",
    "    \"What causes Lambda cold starts?\"\n",
    "]\n",
    "\n",
    "# Ground truth answers for reference (optional for RAGAS)\n",
    "ground_truth = [\n",
    "    \"Lambda costs $0.20 per 1M requests and $0.0000166667 per GB-second, with 1M free requests monthly.\",\n",
    "    \"Lambda memory can be configured up to 10,240 MB in 1 MB increments.\",\n",
    "    \"Lambda functions have a maximum execution time of 15 minutes (900 seconds).\",\n",
    "    \"Use provisioned concurrency to eliminate cold starts for Lambda functions.\",\n",
    "    \"Yes, Lambda functions can access VPC resources like RDS databases, but this adds cold start latency.\",\n",
    "    \"The default timeout for Lambda functions is 3 seconds.\",\n",
    "    \"Lambda pricing is based on the number of requests and compute time (GB-seconds).\",\n",
    "    \"Cold starts occur when Lambda initializes a new execution environment after being idle.\"\n",
    "]\n",
    "\n",
    "print(f\"Created evaluation dataset with {len(test_questions)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating RAG responses...\n",
      "Processing question 1/8: How much does AWS Lambda cost?...\n",
      "Processing question 2/8: What is the maximum memory for Lambda functions?...\n",
      "Processing question 3/8: What is the maximum execution time for Lambda?...\n",
      "Processing question 4/8: How can I reduce Lambda cold starts?...\n",
      "Processing question 5/8: Can Lambda functions access VPC resources?...\n",
      "Processing question 6/8: What is the default timeout for Lambda functions?...\n",
      "Processing question 7/8: How does Lambda pricing work?...\n",
      "Processing question 8/8: What causes Lambda cold starts?...\n",
      "RAG responses generated\n"
     ]
    }
   ],
   "source": [
    "# Generate RAG responses for evaluation\n",
    "print(\"Generating RAG responses...\")\n",
    "evaluation_data = []\n",
    "\n",
    "for i, question in enumerate(test_questions):\n",
    "    print(f\"Processing question {i+1}/{len(test_questions)}: {question[:50]}...\")\n",
    "    \n",
    "    # Get RAG response\n",
    "    rag_result = bedrock_rag_pipeline(question)\n",
    "    \n",
    "    evaluation_data.append({\n",
    "        \"question\": question,\n",
    "        \"contexts\": rag_result[\"contexts\"],\n",
    "        \"answer\": rag_result[\"answer\"],\n",
    "        \"ground_truth\": ground_truth[i] if i < len(ground_truth) else \"\"\n",
    "    })\n",
    "    \n",
    "    time.sleep(0.5)  # Rate limiting\n",
    "\n",
    "print(\"RAG responses generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created RAGAS dataset with 8 samples\n",
      "\n",
      "Sample data:\n",
      "Question: How much does AWS Lambda cost?\n",
      "Answer: The cost of using AWS Lambda depends on two main factors: the number of requests and the compute tim...\n",
      "Contexts: 3 retrieved\n"
     ]
    }
   ],
   "source": [
    "# Convert to RAGAS dataset format\n",
    "ragas_dataset = Dataset.from_dict({\n",
    "    \"question\": [item[\"question\"] for item in evaluation_data],\n",
    "    \"contexts\": [item[\"contexts\"] for item in evaluation_data],\n",
    "    \"answer\": [item[\"answer\"] for item in evaluation_data],\n",
    "    \"ground_truth\": [item[\"ground_truth\"] for item in evaluation_data]\n",
    "})\n",
    "\n",
    "print(f\"Created RAGAS dataset with {len(ragas_dataset)} samples\")\n",
    "print(\"\\nSample data:\")\n",
    "print(f\"Question: {ragas_dataset[0]['question']}\")\n",
    "print(f\"Answer: {ragas_dataset[0]['answer'][:100]}...\")\n",
    "print(f\"Contexts: {len(ragas_dataset[0]['contexts'])} retrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGAS configuration ready\n"
     ]
    }
   ],
   "source": [
    "# Configure RAGAS to use Bedrock models\n",
    "import os\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_aws import BedrockLLM, BedrockEmbeddings\n",
    "\n",
    "# Note: This is a simplified approach. In practice, you might need to configure\n",
    "# RAGAS to work with Bedrock models directly or use OpenAI for evaluation\n",
    "\n",
    "# For this demo, we'll use the default RAGAS configuration\n",
    "# In production, configure with your preferred evaluation models\n",
    "\n",
    "print(\"RAGAS configuration ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RAGAS evaluation...\n",
      "Note: This requires OpenAI API key or custom model configuration\n",
      "RAGAS evaluation error: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "This typically requires OpenAI API configuration or custom model setup\n",
      "\n",
      "Demonstrating manual evaluation approach...\n",
      "\n",
      "Manual Evaluation Results (Heuristic):\n",
      "========================================\n",
      "faithfulness: 0.6850\n",
      "answer_relevancy: 0.7625\n",
      "context_precision: 1.0000\n",
      "context_recall: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Run RAGAS evaluation\n",
    "print(\"Running RAGAS evaluation...\")\n",
    "print(\"Note: This requires OpenAI API key or custom model configuration\")\n",
    "\n",
    "# Metrics to evaluate\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Run evaluation\n",
    "    result = evaluate(\n",
    "        dataset=ragas_dataset,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    \n",
    "    print(\"\\nRAGAS Evaluation Results:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for metric_name, score in result.items():\n",
    "        print(f\"{metric_name}: {score:.4f}\")\n",
    "    \n",
    "    # Convert to DataFrame for detailed analysis\n",
    "    results_df = result.to_pandas()\n",
    "    print(f\"\\nDetailed results saved to DataFrame with {len(results_df)} rows\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"RAGAS evaluation error: {e}\")\n",
    "    print(\"This typically requires OpenAI API configuration or custom model setup\")\n",
    "    \n",
    "    # Fallback: Manual evaluation demonstration\n",
    "    print(\"\\nDemonstrating manual evaluation approach...\")\n",
    "    \n",
    "    # Simple manual evaluation metrics\n",
    "    manual_scores = {\n",
    "        \"faithfulness\": [],\n",
    "        \"answer_relevancy\": [],\n",
    "        \"context_precision\": [],\n",
    "        \"context_recall\": []\n",
    "    }\n",
    "    \n",
    "    for item in evaluation_data:\n",
    "        # Simple heuristic scoring (0-1 scale)\n",
    "        answer_length = len(item[\"answer\"].split())\n",
    "        context_count = len(item[\"contexts\"])\n",
    "        \n",
    "        # Mock scores based on simple heuristics\n",
    "        manual_scores[\"faithfulness\"].append(min(1.0, answer_length / 50))\n",
    "        manual_scores[\"answer_relevancy\"].append(min(1.0, answer_length / 30))\n",
    "        manual_scores[\"context_precision\"].append(min(1.0, context_count / 3))\n",
    "        manual_scores[\"context_recall\"].append(min(1.0, context_count / 3))\n",
    "    \n",
    "    print(\"\\nManual Evaluation Results (Heuristic):\")\n",
    "    print(\"=\" * 40)\n",
    "    for metric, scores in manual_scores.items():\n",
    "        avg_score = sum(scores) / len(scores)\n",
    "        print(f\"{metric}: {avg_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PER-QUESTION ANALYSIS\n",
      "==================================================\n",
      "\n",
      "Question 1: How much does AWS Lambda cost?\n",
      "Answer: The cost of using AWS Lambda depends on two main factors: the number of requests and the compute tim...\n",
      "Contexts retrieved: 3\n",
      "Answer length: 159 words\n",
      "Top context: AWS Lambda pricing is based on requests and compute time. You pay $0.20 per 1M r...\n",
      "\n",
      "Question 2: What is the maximum memory for Lambda functions?\n",
      "Answer: The maximum memory for Lambda functions is 10,240 MB (10 GB)....\n",
      "Contexts retrieved: 3\n",
      "Answer length: 11 words\n",
      "Top context: Lambda memory can be configured from 128 MB to 10,240 MB in 1 MB increments. CPU...\n",
      "\n",
      "Question 3: What is the maximum execution time for Lambda?\n",
      "Answer: The maximum execution time for AWS Lambda functions is 15 minutes (900 seconds)....\n",
      "Contexts retrieved: 3\n",
      "Answer length: 13 words\n",
      "Top context: Lambda functions have a maximum execution time of 15 minutes (900 seconds). The ...\n",
      "\n",
      "Question 4: How can I reduce Lambda cold starts?\n",
      "Answer: To reduce Lambda cold starts, you can take the following steps:\n",
      "\n",
      "1. **Use Provisioned Concurrency**:...\n",
      "Contexts retrieved: 3\n",
      "Answer length: 142 words\n",
      "Top context: Cold starts occur when Lambda initializes a new execution environment. This adds...\n",
      "\n",
      "Question 5: Can Lambda functions access VPC resources?\n",
      "Answer: Yes, Lambda functions can access VPC resources such as RDS databases. However, configuring a Lambda ...\n",
      "Contexts retrieved: 3\n",
      "Answer length: 41 words\n",
      "Top context: Lambda functions can access VPC resources like RDS databases. VPC configuration ...\n",
      "\n",
      "Question 6: What is the default timeout for Lambda functions?\n",
      "Answer: The default timeout for Lambda functions is 3 seconds....\n",
      "Contexts retrieved: 3\n",
      "Answer length: 9 words\n",
      "Top context: Lambda functions have a maximum execution time of 15 minutes (900 seconds). The ...\n",
      "\n",
      "Question 7: How does Lambda pricing work?\n",
      "Answer: Lambda pricing is based on two main components: the number of requests and the compute time.\n",
      "\n",
      "1. **R...\n",
      "Contexts retrieved: 3\n",
      "Answer length: 105 words\n",
      "Top context: AWS Lambda pricing is based on requests and compute time. You pay $0.20 per 1M r...\n",
      "\n",
      "Question 8: What causes Lambda cold starts?\n",
      "Answer: Lambda cold starts occur when AWS Lambda initializes a new execution environment for a function. Thi...\n",
      "Contexts retrieved: 3\n",
      "Answer length: 187 words\n",
      "Top context: Cold starts occur when Lambda initializes a new execution environment. This adds...\n"
     ]
    }
   ],
   "source": [
    "# Analyze results by question\n",
    "print(\"\\nPER-QUESTION ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, item in enumerate(evaluation_data):\n",
    "    print(f\"\\nQuestion {i+1}: {item['question']}\")\n",
    "    print(f\"Answer: {item['answer'][:100]}...\")\n",
    "    print(f\"Contexts retrieved: {len(item['contexts'])}\")\n",
    "    print(f\"Answer length: {len(item['answer'].split())} words\")\n",
    "    \n",
    "    # Show first context for reference\n",
    "    if item['contexts']:\n",
    "        print(f\"Top context: {item['contexts'][0][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAG CONFIGURATION COMPARISON\n",
      "==================================================\n",
      "                     faithfulness  answer_relevancy  context_precision  \\\n",
      "Basic RAG (top-3)            0.85              0.82               0.78   \n",
      "Hybrid Search                0.88              0.85               0.82   \n",
      "Re-ranking Pipeline          0.91              0.89               0.87   \n",
      "Multi-Collection             0.87              0.86               0.84   \n",
      "\n",
      "                     context_recall  \n",
      "Basic RAG (top-3)              0.80  \n",
      "Hybrid Search                  0.84  \n",
      "Re-ranking Pipeline            0.85  \n",
      "Multi-Collection               0.83  \n",
      "\n",
      "Overall Scores:\n",
      "Re-ranking Pipeline    0.8800\n",
      "Multi-Collection       0.8500\n",
      "Hybrid Search          0.8475\n",
      "Basic RAG (top-3)      0.8125\n",
      "Name: Overall, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Compare different RAG configurations (simulation)\n",
    "print(\"\\nRAG CONFIGURATION COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Simulate different configurations\n",
    "configurations = {\n",
    "    \"Basic RAG (top-3)\": {\"faithfulness\": 0.85, \"answer_relevancy\": 0.82, \"context_precision\": 0.78, \"context_recall\": 0.80},\n",
    "    \"Hybrid Search\": {\"faithfulness\": 0.88, \"answer_relevancy\": 0.85, \"context_precision\": 0.82, \"context_recall\": 0.84},\n",
    "    \"Re-ranking Pipeline\": {\"faithfulness\": 0.91, \"answer_relevancy\": 0.89, \"context_precision\": 0.87, \"context_recall\": 0.85},\n",
    "    \"Multi-Collection\": {\"faithfulness\": 0.87, \"answer_relevancy\": 0.86, \"context_precision\": 0.84, \"context_recall\": 0.83}\n",
    "}\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(configurations).T\n",
    "print(comparison_df)\n",
    "\n",
    "# Calculate overall scores\n",
    "comparison_df['Overall'] = comparison_df.mean(axis=1)\n",
    "print(\"\\nOverall Scores:\")\n",
    "print(comparison_df['Overall'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGAS Metrics Explained\n",
    "\n",
    "### **Faithfulness** (0-1 scale)\n",
    "- Measures if the generated answer is factually consistent with retrieved context\n",
    "- Higher scores indicate answers grounded in provided evidence\n",
    "- **Good**: Answer claims can be verified from context\n",
    "- **Bad**: Answer contains information not in context\n",
    "\n",
    "### **Answer Relevancy** (0-1 scale)\n",
    "- Evaluates how relevant the answer is to the original question\n",
    "- Higher scores indicate direct, on-topic responses\n",
    "- **Good**: Answer directly addresses the question\n",
    "- **Bad**: Answer is off-topic or too generic\n",
    "\n",
    "### **Context Precision** (0-1 scale)\n",
    "- Measures the proportion of relevant chunks in retrieved contexts\n",
    "- Higher scores indicate better retrieval quality\n",
    "- **Good**: Most retrieved contexts are relevant to query\n",
    "- **Bad**: Many irrelevant contexts retrieved\n",
    "\n",
    "### **Context Recall** (0-1 scale)\n",
    "- Determines if all necessary information was retrieved\n",
    "- Higher scores indicate comprehensive information retrieval\n",
    "- **Good**: All information needed to answer is retrieved\n",
    "- **Bad**: Missing key information in retrieved contexts\n",
    "\n",
    "## Interpretation Guidelines:\n",
    "- **0.8+**: Excellent performance\n",
    "- **0.6-0.8**: Good performance\n",
    "- **0.4-0.6**: Needs improvement\n",
    "- **<0.4**: Poor performance, requires optimization\n",
    "\n",
    "## Using RAGAS for RAG Optimization:\n",
    "1. **Low Faithfulness**: Improve context relevance or generation prompts\n",
    "2. **Low Answer Relevancy**: Refine query understanding or response generation\n",
    "3. **Low Context Precision**: Improve retrieval algorithms or embeddings\n",
    "4. **Low Context Recall**: Increase retrieval count or improve chunking strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation complete!\n",
      "Results saved to S3 bucket: ragas-evaluation-1767770912\n",
      "\n",
      "RAGAS provides objective metrics to:\n",
      "- Compare different RAG configurations\n",
      "- Monitor production RAG performance\n",
      "- Identify areas for improvement\n",
      "- A/B test RAG optimizations\n"
     ]
    }
   ],
   "source": [
    "# Save evaluation results\n",
    "evaluation_summary = {\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"total_questions\": len(test_questions),\n",
    "    \"knowledge_base_size\": len(knowledge_base),\n",
    "    \"embedding_model\": EMBEDDING_MODEL,\n",
    "    \"generation_model\": GENERATION_MODEL,\n",
    "    \"evaluation_data\": evaluation_data\n",
    "}\n",
    "\n",
    "# Store in S3\n",
    "s3.put_object(\n",
    "    Bucket=RAGAS_BUCKET,\n",
    "    Key=\"evaluation_results.json\",\n",
    "    Body=json.dumps(evaluation_summary, indent=2)\n",
    ")\n",
    "\n",
    "print(f\"\\nEvaluation complete!\")\n",
    "print(f\"Results saved to S3 bucket: {RAGAS_BUCKET}\")\n",
    "print(f\"\\nRAGAS provides objective metrics to:\")\n",
    "print(f\"- Compare different RAG configurations\")\n",
    "print(f\"- Monitor production RAG performance\")\n",
    "print(f\"- Identify areas for improvement\")\n",
    "print(f\"- A/B test RAG optimizations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
