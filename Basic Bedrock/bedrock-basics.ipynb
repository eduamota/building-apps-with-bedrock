{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "270dc010",
   "metadata": {},
   "source": [
    "This notebook demonstrates the fundamental ways to interact with foundation models via Amazon Bedrock using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b339a78a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "import time\n",
    "from IPython.display import Markdown, display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df3ac61",
   "metadata": {},
   "source": [
    "Run the following cell only if running in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca59b068",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = userdata.get('aws_access_key_id')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = userdata.get('aws_secret_access_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e345b4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the Bedrock client\n",
    "# Make sure you have configured your AWS credentials\n",
    "# Change the region if needed\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "print(\"Bedrock client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667d1455",
   "metadata": {},
   "source": [
    "Let's define some helper functions to work with Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f3159c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def invoke_model(model_id, prompt, max_tokens=512, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Invokes a foundation model through Amazon Bedrock.\n",
    "    \n",
    "    Parameters:\n",
    "    - model_id: The identifier for the model (e.g., \"anthropic.claude-3-haiku-20240307\")\n",
    "    - prompt: The prompt text to send to the model\n",
    "    - max_tokens: Maximum number of tokens to generate\n",
    "    - temperature: Controls randomness (0.0 = deterministic, 1.0 = creative)\n",
    "    \n",
    "    Returns:\n",
    "    - The model's response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create the appropriate request body based on model provider\n",
    "        if \"anthropic.claude\" in model_id:\n",
    "            # Anthropic Claude models\n",
    "            request_body = {\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        elif \"meta.llama\" in model_id:\n",
    "            # Meta Llama models\n",
    "            request_body = {\n",
    "                \"prompt\": prompt,\n",
    "                \"max_gen_len\": max_tokens,\n",
    "                \"temperature\": temperature\n",
    "            }\n",
    "        elif \"amazon.titan\" in model_id:\n",
    "            # Amazon Titan models\n",
    "            request_body = {\n",
    "                \"inputText\": prompt,\n",
    "                \"textGenerationConfig\": {\n",
    "                    \"maxTokenCount\": max_tokens,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"topP\": 0.9\n",
    "                }\n",
    "            }\n",
    "        elif \"ai21.j2\" in model_id:\n",
    "            # AI21 Jurassic models\n",
    "            request_body = {\n",
    "                \"prompt\": prompt,\n",
    "                \"maxTokens\": max_tokens,\n",
    "                \"temperature\": temperature\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Model {model_id} not explicitly supported in this demo.\")\n",
    "            return None\n",
    "            \n",
    "        # Convert the request body to JSON string\n",
    "        body = json.dumps(request_body)\n",
    "        \n",
    "        # Invoke the model\n",
    "        print(f\"Sending request to {model_id}...\")\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=body\n",
    "        )\n",
    "        \n",
    "        # Parse and return the response\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        \n",
    "        # Extract the generated text based on model provider\n",
    "        if \"anthropic.claude\" in model_id:\n",
    "            return response_body[\"content\"][0][\"text\"]\n",
    "        elif \"meta.llama\" in model_id:\n",
    "            return response_body[\"generation\"]\n",
    "        elif \"amazon.titan\" in model_id:\n",
    "            return response_body[\"results\"][0][\"outputText\"]\n",
    "        elif \"ai21.j2\" in model_id:\n",
    "            return response_body[\"completions\"][0][\"data\"][\"text\"]\n",
    "        else:\n",
    "            return response_body\n",
    "            \n",
    "    except ClientError as e:\n",
    "        print(f\"Error invoking model: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d80650c",
   "metadata": {},
   "source": [
    "## 1. Basic Model Invocation\n",
    "\n",
    "Let's start with a simple example of invoking a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e2389",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define our model and prompt\n",
    "model_id = \"anthropic.claude-3-haiku-20240307\"  # Change to any Bedrock model you have access to\n",
    "prompt = \"Explain the concept of foundation models in AI to a software developer.\"\n",
    "\n",
    "# Call the model\n",
    "response = invoke_model(model_id, prompt)\n",
    "\n",
    "# Display the response in markdown for better readability\n",
    "display(Markdown(f\"## Response from {model_id}\"))\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7012db94",
   "metadata": {},
   "source": [
    "## 2. Comparing Different Models\n",
    "\n",
    "Now let's see how to compare responses from different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864bdef2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def compare_models(prompt, models):\n",
    "    \"\"\"\n",
    "    Compare responses from multiple models for the same prompt\n",
    "    \n",
    "    Parameters:\n",
    "    - prompt: The prompt to send to all models\n",
    "    - models: List of model IDs to compare\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary mapping model IDs to their responses\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for model_id in models:\n",
    "        print(f\"Querying {model_id}...\")\n",
    "        response = invoke_model(model_id, prompt)\n",
    "        results[model_id] = response\n",
    "        # Add a small delay to avoid rate limiting\n",
    "        time.sleep(1)\n",
    "    return results\n",
    "\n",
    "# Define our prompt and models to compare\n",
    "comparison_prompt = \"What are the key features of Amazon Bedrock?\"\n",
    "models_to_compare = [\n",
    "    \"anthropic.claude-3-haiku-20240307\",  # Claude 3 Haiku\n",
    "    \"amazon.titan-text-express-v1\"        # Amazon Titan Text\n",
    "]\n",
    "\n",
    "comparison_results = compare_models(comparison_prompt, models_to_compare)\n",
    "\n",
    "# Display results in markdown\n",
    "for model, result in comparison_results.items():\n",
    "    display(Markdown(f\"## Response from {model}\"))\n",
    "    display(Markdown(result))\n",
    "    display(Markdown(\"---\"))\n",
    "\n",
    "print(\"Note: Model comparison code is ready but commented out to avoid unnecessary API calls.\")\n",
    "print(\"During your actual demo, you can uncomment the code above to run a live comparison.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39e2e30",
   "metadata": {},
   "source": [
    "## 3. Using System Prompts\n",
    "\n",
    "System prompts help guide the model's behavior. Let's see how to use them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a06344f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def demonstrate_system_prompt(model_id=\"anthropic.claude-3-haiku-20240307\"):\n",
    "    \"\"\"\n",
    "    Demonstrates how to use system prompts with Claude models\n",
    "    \"\"\"\n",
    "    try:\n",
    "        system_prompt = \"You are a helpful AI assistant specialized in AWS services. Keep your answers concise and technical.\"\n",
    "        user_prompt = \"What is Amazon Bedrock and why would I use it?\"\n",
    "        \n",
    "        print(f\"System prompt: '{system_prompt}'\")\n",
    "        print(f\"User prompt: '{user_prompt}'\")\n",
    "        \n",
    "        request_body = {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 500,\n",
    "            \"temperature\": 0.7,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(request_body)\n",
    "        )\n",
    "        \n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        return response_body[\"content\"][0][\"text\"]\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"Error invoking model with system prompt: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run the system prompt demo\n",
    "system_response = demonstrate_system_prompt()\n",
    "display(Markdown(f\"## Response with System Prompt\"))\n",
    "display(Markdown(system_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d69c6a",
   "metadata": {},
   "source": [
    "## 4. Streaming Responses\n",
    "\n",
    "For more responsive applications, you can stream responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce0b0de",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def demonstrate_streaming(model_id=\"anthropic.claude-3-haiku-20240307\", prompt=\"Tell me about AWS Bedrock in 3 sentences\"):\n",
    "    \"\"\"\n",
    "    Demonstrates streaming output from a Bedrock model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Sending streaming request to {model_id} with prompt: '{prompt}'\")\n",
    "        \n",
    "        request_body = {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 300,\n",
    "            \"temperature\": 0.7,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        response = bedrock_runtime.invoke_model_with_response_stream(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(request_body)\n",
    "        )\n",
    "        \n",
    "        stream = response.get('body')\n",
    "        \n",
    "        # In a notebook, we'll collect the chunks and then display them\n",
    "        # In a real application, you would process these chunks as they arrive\n",
    "        print(\"Receiving streaming response (chunks collected and displayed at the end):\")\n",
    "        chunks = []\n",
    "        for event in stream:\n",
    "            chunk = event.get('chunk')\n",
    "            if chunk:\n",
    "                chunk_data = json.loads(chunk.get('bytes').decode())\n",
    "                if \"content\" in chunk_data and chunk_data[\"content\"]:\n",
    "                    content = chunk_data[\"content\"][0][\"text\"]\n",
    "                    chunks.append(content)\n",
    "                    # For a real streaming demo, you could use this instead:\n",
    "                    # display(content, display_id='streaming_output', update=True)\n",
    "        \n",
    "        return \"\".join(chunks)\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"Error with streaming: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run the streaming demo\n",
    "streaming_response = demonstrate_streaming()\n",
    "display(Markdown(f\"## Complete Streaming Response\"))\n",
    "display(Markdown(streaming_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bc6f0a",
   "metadata": {},
   "source": [
    "## 5. Practical Example: Creating Documentation\n",
    "\n",
    "Let's put everything together in a practical example where we ask the model to create documentation for an AWS service:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59971222",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_documentation(service_name, model_id=\"anthropic.claude-3-haiku-20240307\"):\n",
    "    \"\"\"\n",
    "    Use Bedrock to generate documentation for an AWS service\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"\n",
    "    You are an AWS documentation expert. Create concise, accurate, and helpful documentation.\n",
    "    Format your response with markdown, including:\n",
    "    - A brief service overview\n",
    "    - Key features (as a bulleted list)\n",
    "    - A simple code example\n",
    "    - Common use cases\n",
    "    \"\"\"\n",
    "    \n",
    "    user_prompt = f\"Create documentation for {service_name}.\"\n",
    "    \n",
    "    request_body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0.5,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=model_id,\n",
    "        body=json.dumps(request_body)\n",
    "    )\n",
    "    \n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    return response_body[\"content\"][0][\"text\"]\n",
    "\n",
    "bedrock_docs = create_documentation(\"Amazon Bedrock\")\n",
    "display(Markdown(f\"# Generated Documentation\"))\n",
    "display(Markdown(bedrock_docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0feba58",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated several ways to interact with Amazon Bedrock:\n",
    "\n",
    "1. Basic model invocation\n",
    "2. Comparing responses from different models\n",
    "3. Using system prompts to guide model behavior\n",
    "4. Streaming responses for more responsive applications\n",
    "5. A practical example of generating documentation\n",
    "\n",
    "These patterns form the foundation for building more complex GenAI applications on AWS.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
